{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SUNY DHSU Machine Learning Course Begins July 7th, 2025. Important Dates: 7/7: Class Begins 8/4: Deadling to provide research project plan 8/4,8/7,8/12,8/17: Office Hours for Research Projects 8/21: Research Presentations Download workshop practical data! Attendees should complete the pre-course checklist! Download tutorial materials here. Join the workshop google-group here. This short course will equip scientists with the tools and approaches required to use ML in their own reserach. This course will include both applied and theoretical topics in ML/LLM research, delivered across multiple lectures, seminars and computational practicals. In order to ensure that students are properly prepared for the course, then should complete our \"Pre-Course Guide\" first, by going through each of the sections shown on the left of the screen, starting with 'Checklist'. For additional questions, don't hesitate to email the speakers: 1. Thomas Wallach - [thomas.wallach at downstate.edu] 2. Olumide Arigbede - [olumide.arigbede at downstate.edu] 3. Bryce Schroeder - [bryce.schroeder at downstate.edu] 4. Tade Souaiaia - [tade.souaiaia at downstate.edu]","title":"Home"},{"location":"#suny-dhsu-machine-learning-course-begins-july-7th-2025","text":"","title":"SUNY DHSU Machine Learning Course Begins July 7th, 2025."},{"location":"#important-dates","text":"7/7: Class Begins 8/4: Deadling to provide research project plan 8/4,8/7,8/12,8/17: Office Hours for Research Projects 8/21: Research Presentations Download workshop practical data! Attendees should complete the pre-course checklist! Download tutorial materials here. Join the workshop google-group here. This short course will equip scientists with the tools and approaches required to use ML in their own reserach. This course will include both applied and theoretical topics in ML/LLM research, delivered across multiple lectures, seminars and computational practicals. In order to ensure that students are properly prepared for the course, then should complete our \"Pre-Course Guide\" first, by going through each of the sections shown on the left of the screen, starting with 'Checklist'. For additional questions, don't hesitate to email the speakers: 1. Thomas Wallach - [thomas.wallach at downstate.edu] 2. Olumide Arigbede - [olumide.arigbede at downstate.edu] 3. Bryce Schroeder - [bryce.schroeder at downstate.edu] 4. Tade Souaiaia - [tade.souaiaia at downstate.edu]","title":"Important Dates:"},{"location":"misc_linux/","text":"Linux for Windows Users For windows users, a single WSL command can be used to install linux. Detailed information on WSL can be found on the microsoft website and step-by-step video obstructions can be found here . These steps involve: Installation Using Administrator Mode to Installing Linux Open PowerShell or Windows Command Prompt by right-clicking and selecting \"Run as administrator\", and type: wsl --install -d Ubuntu This command will enable the features necessary to run WSL, restart your computer, and install the Ubuntu distribution of Linux. After it has been installed you will need to create a new username and password, to learn more about this please see microsoft best practices Setup Once Linux has been installed and you have created a new account you can run Ubuntu using windows terminal or by going to the start menu and typing \"Ubuntu\". Additionally, you should see that Ubuntu has been added to your applications and that there is a Ubuntu Folder on your computer (alongside Desktop, Downloads, etc). Double clicking the Ubuntu app will open up the Ubuntu terminal in the Ubuntu folder . You should see many sub-folders within the Ubuntu folder (such as bin, home, etc, etc.) . You can view them using terminal commands or by double-clicking on them. If you go into the home/ folder you should see another folder with your username. This folder ( home/(your username)/ ) is where you should store your workshop folders and run programs during the workshop. Once you have successfully installed Ubuntu, you can follow the rest of the pre-workshop tutorial using the Linux instructions.","title":"Linux For Windows"},{"location":"misc_linux/#linux-for-windows-users","text":"For windows users, a single WSL command can be used to install linux. Detailed information on WSL can be found on the microsoft website and step-by-step video obstructions can be found here . These steps involve:","title":"Linux for Windows Users"},{"location":"misc_linux/#installation","text":"Using Administrator Mode to Installing Linux Open PowerShell or Windows Command Prompt by right-clicking and selecting \"Run as administrator\", and type: wsl --install -d Ubuntu This command will enable the features necessary to run WSL, restart your computer, and install the Ubuntu distribution of Linux. After it has been installed you will need to create a new username and password, to learn more about this please see microsoft best practices","title":"Installation"},{"location":"misc_linux/#setup","text":"Once Linux has been installed and you have created a new account you can run Ubuntu using windows terminal or by going to the start menu and typing \"Ubuntu\". Additionally, you should see that Ubuntu has been added to your applications and that there is a Ubuntu Folder on your computer (alongside Desktop, Downloads, etc). Double clicking the Ubuntu app will open up the Ubuntu terminal in the Ubuntu folder . You should see many sub-folders within the Ubuntu folder (such as bin, home, etc, etc.) . You can view them using terminal commands or by double-clicking on them. If you go into the home/ folder you should see another folder with your username. This folder ( home/(your username)/ ) is where you should store your workshop folders and run programs during the workshop. Once you have successfully installed Ubuntu, you can follow the rest of the pre-workshop tutorial using the Linux instructions.","title":"Setup"},{"location":"misc_plink_problem/","text":"MacOs Permission Errors If you see this error, Do not click: Move To Trash . Instead click on the top right hand corner of the box, or stop running whatever program you are using and follow the instructions below to give your system permission to run downloaded software. to give your system permission to run downloaded software. Shortcut: Users report that this shortcut can also be used to change program permissions: User finder navigate to the file in question. Right click on the icon and select open . A warning will pop-up. Click open and this program can be run in the future. 1- Change Default Settings By default macOS allows you to open apps from the official Mac App Store only, If you have this set as your default you can change this if you: Open System Preferences. Go to the Security & Privacy tab. Click on the lock and enter your password. Change settings to include identified developers (see below): 2- Allow Exceptions Expanding permissions to include \"identified developers\" is required but not sufficient. To obtain further permission to run Plink or any other unapproved program you can: Open System Preferences. Go to Security & Privacy and select the General tab. If this has happened within the hour, this page will give you an override button to open Open Anyway . Enter you password as above and click this button. You will be asked to once more which will create an exception allowing you to run Plink in the future. BridgePRS specific errors If this problem has occured when running bridgePRS and you have moved plink to the trash you will have to recover it. Additionally the empty files created by a failed attempt to run Plink can cause problems if BridgePRS tries to recover your progress. Restarting a bridgePRS run You can avoid this problem by manually deleting your output directory and starting over, or by using the restart flag: $./bridgePRS pipeline go -o out1 --config_files data/afr.config data/eur.config --phenotype y --restart This will force bridgePRS to restart every subprogram from the beginning.","title":"Mac Security"},{"location":"misc_plink_problem/#macos-permission-errors","text":"If you see this error, Do not click: Move To Trash . Instead click on the top right hand corner of the box, or stop running whatever program you are using and follow the instructions below to give your system permission to run downloaded software. to give your system permission to run downloaded software. Shortcut: Users report that this shortcut can also be used to change program permissions: User finder navigate to the file in question. Right click on the icon and select open . A warning will pop-up. Click open and this program can be run in the future.","title":"MacOs Permission Errors"},{"location":"misc_plink_problem/#1-change-default-settings","text":"By default macOS allows you to open apps from the official Mac App Store only, If you have this set as your default you can change this if you: Open System Preferences. Go to the Security & Privacy tab. Click on the lock and enter your password. Change settings to include identified developers (see below):","title":"1- Change Default Settings"},{"location":"misc_plink_problem/#2-allow-exceptions","text":"Expanding permissions to include \"identified developers\" is required but not sufficient. To obtain further permission to run Plink or any other unapproved program you can: Open System Preferences. Go to Security & Privacy and select the General tab. If this has happened within the hour, this page will give you an override button to open Open Anyway . Enter you password as above and click this button. You will be asked to once more which will create an exception allowing you to run Plink in the future.","title":"2- Allow Exceptions"},{"location":"misc_plink_problem/#bridgeprs-specific-errors","text":"If this problem has occured when running bridgePRS and you have moved plink to the trash you will have to recover it. Additionally the empty files created by a failed attempt to run Plink can cause problems if BridgePRS tries to recover your progress. Restarting a bridgePRS run You can avoid this problem by manually deleting your output directory and starting over, or by using the restart flag: $./bridgePRS pipeline go -o out1 --config_files data/afr.config data/eur.config --phenotype y --restart This will force bridgePRS to restart every subprogram from the beginning.","title":"BridgePRS specific errors"},{"location":"prep_list/","text":"Introduction Here we present a preparation guide to get students sufficiently prepared before the course begins. In this guide we help students set up the operating system (Linux or macOS), provide tutorials for Bash, R, and Python for students who are unfamiliar or need a refresher, and help students verify that their system is capable of running the software necessary for the workshop. Reminder: All students must complete pre-workship testing and verification At the end of this guide, students are led through a series of software tests . The tests are required to verify that student laptops are properly set-up before the workshop. Students unable to properly configure laptops should msg the workshop google group (see below.) How To: Join the workshop google-group (group name: sunyml) Students without a google account (any email can be used) can create one here: creating a google account Use this account to join the workshop group here or search \"sunyml\" the main groups page. Post questions/answers to configuration issues and stay tuned for important announcements. Checklist This checklist will help guide you through the pre-workshop requirements: Operating System : Confirm or install a compatible operating system. Terminal Setup : Setting up the terminal, creating workshop directory. Software Installations : Confirm/Install R, Python3, and Plink as well their as required libraries. Software Tutorials : If unfamiliar with bash, R, or python, please complete the included tutorials. Verification/Testing : Download materials, test that everything is in working order ( Required ).","title":"Checklist"},{"location":"prep_list/#introduction","text":"Here we present a preparation guide to get students sufficiently prepared before the course begins. In this guide we help students set up the operating system (Linux or macOS), provide tutorials for Bash, R, and Python for students who are unfamiliar or need a refresher, and help students verify that their system is capable of running the software necessary for the workshop. Reminder: All students must complete pre-workship testing and verification At the end of this guide, students are led through a series of software tests . The tests are required to verify that student laptops are properly set-up before the workshop. Students unable to properly configure laptops should msg the workshop google group (see below.) How To: Join the workshop google-group (group name: sunyml) Students without a google account (any email can be used) can create one here: creating a google account Use this account to join the workshop group here or search \"sunyml\" the main groups page. Post questions/answers to configuration issues and stay tuned for important announcements.","title":"Introduction"},{"location":"prep_list/#checklist","text":"This checklist will help guide you through the pre-workshop requirements: Operating System : Confirm or install a compatible operating system. Terminal Setup : Setting up the terminal, creating workshop directory. Software Installations : Confirm/Install R, Python3, and Plink as well their as required libraries. Software Tutorials : If unfamiliar with bash, R, or python, please complete the included tutorials. Verification/Testing : Download materials, test that everything is in working order ( Required ).","title":"Checklist"},{"location":"prep_os/","text":"Operating System In this workshop, students will run multiple different statistical genetic software programs. Large-scale genetic data analysis is almost always performed using a server or cloud-based system running a Linux operating system. Therefore, a Linux-based operating system is highly recommended for this workshop, and users running MacOS ( see here ) or Windows ( see here ) are encouraged to install Linux on their laptop if possible. However, the workshop can be completed in the MacOS environments (although there are likely to be more compatibility problems with MacOS). Microsoft Windows users ARE REQUIRED to install Linux and, therefore, we provide detailed instructions to do this. This workshop is designed for either of the following operating systems: Linux: A Debian compatible Linux based operating system (Ubuntu, Mint, etc). macOS: A recent version (11+) of the Desktop OS. Microsoft Windows is not supported, but instructions to install Linux can be found here.","title":"Operating System"},{"location":"prep_os/#operating-system","text":"In this workshop, students will run multiple different statistical genetic software programs. Large-scale genetic data analysis is almost always performed using a server or cloud-based system running a Linux operating system. Therefore, a Linux-based operating system is highly recommended for this workshop, and users running MacOS ( see here ) or Windows ( see here ) are encouraged to install Linux on their laptop if possible. However, the workshop can be completed in the MacOS environments (although there are likely to be more compatibility problems with MacOS). Microsoft Windows users ARE REQUIRED to install Linux and, therefore, we provide detailed instructions to do this. This workshop is designed for either of the following operating systems: Linux: A Debian compatible Linux based operating system (Ubuntu, Mint, etc). macOS: A recent version (11+) of the Desktop OS. Microsoft Windows is not supported, but instructions to install Linux can be found here.","title":"Operating System"},{"location":"prep_software/","text":"Required System Software This workshop requires that the following software be installed and tested before the workshop: Name/Link Description Additional Requirements 1. R Most popular analysis program in statistical genetics Yes (Specific Libraries). 2. Python 3 . Multi purpose Programming Language Yes (The matplotlib library). Both popular computer languages, R and python3 will be used during the workshop. Here we will provide a guide for installation of both languages and their associated libraries. 1. R (and associated packages) R is the most popular analysis program in statistical genetics, and required for most genetic analysis. We recommend that MacOS users download an up-to-date (4.4.1) version directly from the R website and using the R installer. If you are having trouble we recommend this video tutorial . While Linux users can also find a suitable version on the website, we recommend using the package installer that comes with Debian based distributions (Debian, Ubuntu, Mint, etc) and downloading R directly using the following terminal commands: sudo apt install r-base # to install R sudo apt install build-essential # to install the essential packages Important: Additional R Packages Are Required This workshop requires the following non-standard R packages: BEDMatrix, boot, data.table, doMC, glmnet, MASS, optparse, parallel, and R.utils These packages can be installed by opening up an R session from within the terminal, by typing: R And typing the following command: install.packages(c(\"BEDMatrix\",\"boot\",\"data.table\",\"doMC\",\"glmnet\",\"MASS\",\"optparse\",\"parallel\",\"R.utils\")) After you have finished installation and testing of R, please consider completing our R-tutorial to better familiarize yourself with some basic analysis commands. 2. Python3 (and matplotlib) Python3 is a popular multiuse programming language. This workshop requires Python3 and the matplotlib library. Any version of Python3 is acceptable but a newer version Python3.10.10+ is recommended. Many systems come installed with Python3 by default, but it can be downloaded from the python website . The matplotlib package can be found here . Users of macOs can find detailed instructions on how to install python3 by visiting the following video link that we find very helpful. After installing python3, macOs users can install matplotlib using pip: python3 -m ensurepip sudo python3 -m pip install -U matplotlib For Linux we again recommend that you use the package installer to install python3 and matplotlib sudo apt install python3 # to install python3 python3 --version # to verify that it worked sudo apt-get install python3-matplotlib # to install matplotlib After you have finished with this section, don't forget to complete our Python tutorial to familiarize yourself with some basic commands.","title":"Software Installs"},{"location":"prep_software/#required-system-software","text":"This workshop requires that the following software be installed and tested before the workshop: Name/Link Description Additional Requirements 1. R Most popular analysis program in statistical genetics Yes (Specific Libraries). 2. Python 3 . Multi purpose Programming Language Yes (The matplotlib library). Both popular computer languages, R and python3 will be used during the workshop. Here we will provide a guide for installation of both languages and their associated libraries.","title":"Required System Software"},{"location":"prep_software/#1-r-and-associated-packages","text":"R is the most popular analysis program in statistical genetics, and required for most genetic analysis. We recommend that MacOS users download an up-to-date (4.4.1) version directly from the R website and using the R installer. If you are having trouble we recommend this video tutorial . While Linux users can also find a suitable version on the website, we recommend using the package installer that comes with Debian based distributions (Debian, Ubuntu, Mint, etc) and downloading R directly using the following terminal commands: sudo apt install r-base # to install R sudo apt install build-essential # to install the essential packages Important: Additional R Packages Are Required This workshop requires the following non-standard R packages: BEDMatrix, boot, data.table, doMC, glmnet, MASS, optparse, parallel, and R.utils These packages can be installed by opening up an R session from within the terminal, by typing: R And typing the following command: install.packages(c(\"BEDMatrix\",\"boot\",\"data.table\",\"doMC\",\"glmnet\",\"MASS\",\"optparse\",\"parallel\",\"R.utils\")) After you have finished installation and testing of R, please consider completing our R-tutorial to better familiarize yourself with some basic analysis commands.","title":"1. R (and associated packages)"},{"location":"prep_software/#2-python3-and-matplotlib","text":"Python3 is a popular multiuse programming language. This workshop requires Python3 and the matplotlib library. Any version of Python3 is acceptable but a newer version Python3.10.10+ is recommended. Many systems come installed with Python3 by default, but it can be downloaded from the python website . The matplotlib package can be found here . Users of macOs can find detailed instructions on how to install python3 by visiting the following video link that we find very helpful. After installing python3, macOs users can install matplotlib using pip: python3 -m ensurepip sudo python3 -m pip install -U matplotlib For Linux we again recommend that you use the package installer to install python3 and matplotlib sudo apt install python3 # to install python3 python3 --version # to verify that it worked sudo apt-get install python3-matplotlib # to install matplotlib After you have finished with this section, don't forget to complete our Python tutorial to familiarize yourself with some basic commands.","title":"2. Python3 (and matplotlib)"},{"location":"prep_terminal/","text":"Setting up the Terminal Terminal programs allow you to navigate the Unix-Based OS, a simple system made up of files and folders. For this workshop, basic familiarity is required. Here we will go over setting up your terminal. If you are unfamiliar with the terminal afterwards it is recommended you complete our bash tutorial . To begin open up a terminal window, by: macOS: Searching for \"terminal\" on top toolbar, or going to Applications/Utilities/ and clicking the icon. Linux: Opening the lower left hand start menu, typing \"terminal\" and clicking on the icon. Note: Users running Ubuntu in Windows can follow the Linux directions after double clicking the Ubuntu App icon to bring up the terminal A window will open up that looks something like this: Bash Type: cd ~ The cd command (learn more) stands for change directory, and ~ represents a shortcut to your \"home\" directory. After typing this command, you are \"in\" your home directory. Now type: pwd The pwd command returns your current \"path\", which represents the name and location of your home directory. Depending on your machine typing pwd from your home directory may return: /home/(your username) (most Linux environments) /Users/(your username) (older macOs, shared systems) Something else. Because the name, location, and syntax differs from computer to computer, we use $HOME as a universal shortcut to represent your home directory. The command ls (learn_more) lists the files and folders in your home directory. These files and folders in the home directory will differ from computer to computer, but most filesystems will have a Desktop and Downloads folder in the home directory: Often the home directory of the filesystem can be quite crowded, which is why we would like to carry out this tutorial in a new directory specifically for the workshop. To create this directory, type: mkdir sunyml The mkdir command makes a new directory called sunyml that is located in your home directory: Next we can move to this newly created directory by typing: cd sunyml Now we will create a file using the program nano by typing: nano test.sh This command will open up the nano text editor: On the top line type: echo \"hello world\" Then save the file using Ctrl-O and press enter, and quit using Ctrl-X and pressing enter. Now type: ls Do you see the file you just created? To see the contents of the file you just created you can type: cat test.sh Do you know that the file you just created can also run as program? To execute the bash code that you have created you can type: bash test.sh Congratulations, you have now navigated the filesystem, created a directory, created a file and executed a program! You may feel like somewhat of an expert. However, if you still feel unfamiliar with shell scripting, please consider completing our longer bash tutorial . Otherwise, proceed to the next step, where you will install all system wide software necessary for the workshop.","title":"Terminal Setup"},{"location":"prep_terminal/#setting-up-the-terminal","text":"Terminal programs allow you to navigate the Unix-Based OS, a simple system made up of files and folders. For this workshop, basic familiarity is required. Here we will go over setting up your terminal. If you are unfamiliar with the terminal afterwards it is recommended you complete our bash tutorial . To begin open up a terminal window, by: macOS: Searching for \"terminal\" on top toolbar, or going to Applications/Utilities/ and clicking the icon. Linux: Opening the lower left hand start menu, typing \"terminal\" and clicking on the icon. Note: Users running Ubuntu in Windows can follow the Linux directions after double clicking the Ubuntu App icon to bring up the terminal A window will open up that looks something like this:","title":"Setting up the Terminal"},{"location":"prep_terminal/#bash","text":"Type: cd ~ The cd command (learn more) stands for change directory, and ~ represents a shortcut to your \"home\" directory. After typing this command, you are \"in\" your home directory. Now type: pwd The pwd command returns your current \"path\", which represents the name and location of your home directory. Depending on your machine typing pwd from your home directory may return: /home/(your username) (most Linux environments) /Users/(your username) (older macOs, shared systems) Something else. Because the name, location, and syntax differs from computer to computer, we use $HOME as a universal shortcut to represent your home directory. The command ls (learn_more) lists the files and folders in your home directory. These files and folders in the home directory will differ from computer to computer, but most filesystems will have a Desktop and Downloads folder in the home directory: Often the home directory of the filesystem can be quite crowded, which is why we would like to carry out this tutorial in a new directory specifically for the workshop. To create this directory, type: mkdir sunyml The mkdir command makes a new directory called sunyml that is located in your home directory: Next we can move to this newly created directory by typing: cd sunyml Now we will create a file using the program nano by typing: nano test.sh This command will open up the nano text editor: On the top line type: echo \"hello world\" Then save the file using Ctrl-O and press enter, and quit using Ctrl-X and pressing enter. Now type: ls Do you see the file you just created? To see the contents of the file you just created you can type: cat test.sh Do you know that the file you just created can also run as program? To execute the bash code that you have created you can type: bash test.sh Congratulations, you have now navigated the filesystem, created a directory, created a file and executed a program! You may feel like somewhat of an expert. However, if you still feel unfamiliar with shell scripting, please consider completing our longer bash tutorial . Otherwise, proceed to the next step, where you will install all system wide software necessary for the workshop.","title":"Bash"},{"location":"prep_testing/","text":"Preworkshop Testing Downloading materials For Linux: Link. For macOs: Link. By clicking the link, then the download ( down-arrow ) icon at the top of the screen. If a message comes up that the file can't be scanned for viruses, please click \"download anyway\". You should already have a folder in your home directory named sunyml that you created during the terminal part of this guide by typing \"mkdir ~/sunyml\". If you haven't created this folder, please do so now. Next, move to this directory by typing: cd ~/sunyml Next unzip the downloaded workshop materials and move them to this directory. This can be down by right clicking on the folder to unzip and then dragging it into the sunyml folder, or it can be done in the terminal. In Linux this is accomplished by typing: tar -xvzf ~/Downloads/preworkshop_materials_linux.tar.gz -C ~/sunyml cd ~/sunyml/preworkshop_materials_linux MacOs safari sometimes unzips downloaded files for you, so macOs users should first type the following: ls ~/Downloads/preworkshop_materials_mac.* If the command returns a file with a \".tar\" ending, then type: tar -xvf ~/Downloads/preworkshop_materials_mac.tar -C ~/sunyml Otherwise, if the command returns a file with a \".tar.gz\" ending, type: tar -xvzf ~/Downloads/preworkshop_materials_mac.tar.gz -C ~/sunyml To unzip the folder and move it to the appropriate directory. Finally, move to the appropriate directory to begin testing: cd ~/sunyml/preworkshop_materials_mac Testing Software Warning For macOs Users: MacOs often block executables if they are not approved from the app store. If you see an error similar to this, when trying to run plink, PRSice, or bridgePRS, DO NOT DELETE THE FILE . You must change your system settings to allow downloaded software. To learn how to do so, please click here . Plink From the preworkshop directory, navigate into the Plink directory. cd Plink and type the following command: ./code/plink -h If no error is observed and a list of options are displayed then your computer is ready to run plink. If you are not already familiar with plink, now is a great time to use the data found in Plink/tutorials/sample_data/ to run the PLINK tutorial . Testing PRSice Please navigate to the correct directory: cd ~/sunyml/preworkshop_materials_(mac/linux)/PRSice and type the following command: ./code/PRSice -h # macOs users If no error is observed and a list of options are displayed then your computer is ready to run PRSice. Testing bridgePRS To verify that bridgePRS is able to run navigate to the folder: cd ~/sunyml/preworkshop_materials_(mac/linux)/BridgePRS and type the command: ./bridgePRS You should see bridge art. If this command works, then type the following command: ./bridgePRS tools check-requirements To confirm that your system is up to date, all libraries are installed and you are ready to run bridgePRS.","title":"Testing"},{"location":"prep_testing/#preworkshop-testing","text":"","title":"Preworkshop Testing"},{"location":"prep_testing/#downloading-materials","text":"For Linux: Link. For macOs: Link. By clicking the link, then the download ( down-arrow ) icon at the top of the screen. If a message comes up that the file can't be scanned for viruses, please click \"download anyway\". You should already have a folder in your home directory named sunyml that you created during the terminal part of this guide by typing \"mkdir ~/sunyml\". If you haven't created this folder, please do so now. Next, move to this directory by typing: cd ~/sunyml Next unzip the downloaded workshop materials and move them to this directory. This can be down by right clicking on the folder to unzip and then dragging it into the sunyml folder, or it can be done in the terminal. In Linux this is accomplished by typing: tar -xvzf ~/Downloads/preworkshop_materials_linux.tar.gz -C ~/sunyml cd ~/sunyml/preworkshop_materials_linux MacOs safari sometimes unzips downloaded files for you, so macOs users should first type the following: ls ~/Downloads/preworkshop_materials_mac.* If the command returns a file with a \".tar\" ending, then type: tar -xvf ~/Downloads/preworkshop_materials_mac.tar -C ~/sunyml Otherwise, if the command returns a file with a \".tar.gz\" ending, type: tar -xvzf ~/Downloads/preworkshop_materials_mac.tar.gz -C ~/sunyml To unzip the folder and move it to the appropriate directory. Finally, move to the appropriate directory to begin testing: cd ~/sunyml/preworkshop_materials_mac","title":"Downloading materials"},{"location":"prep_testing/#testing-software","text":"Warning For macOs Users: MacOs often block executables if they are not approved from the app store. If you see an error similar to this, when trying to run plink, PRSice, or bridgePRS, DO NOT DELETE THE FILE . You must change your system settings to allow downloaded software. To learn how to do so, please click here .","title":"Testing Software"},{"location":"prep_testing/#plink","text":"From the preworkshop directory, navigate into the Plink directory. cd Plink and type the following command: ./code/plink -h If no error is observed and a list of options are displayed then your computer is ready to run plink. If you are not already familiar with plink, now is a great time to use the data found in Plink/tutorials/sample_data/ to run the PLINK tutorial .","title":"Plink"},{"location":"prep_testing/#testing-prsice","text":"Please navigate to the correct directory: cd ~/sunyml/preworkshop_materials_(mac/linux)/PRSice and type the following command: ./code/PRSice -h # macOs users If no error is observed and a list of options are displayed then your computer is ready to run PRSice.","title":"Testing PRSice"},{"location":"prep_testing/#testing-bridgeprs","text":"To verify that bridgePRS is able to run navigate to the folder: cd ~/sunyml/preworkshop_materials_(mac/linux)/BridgePRS and type the command: ./bridgePRS You should see bridge art. If this command works, then type the following command: ./bridgePRS tools check-requirements To confirm that your system is up to date, all libraries are installed and you are ready to run bridgePRS.","title":"Testing bridgePRS"},{"location":"tut_R/","text":"Introduction to R R is a useful programming language that allows us to perform a variety of statis- tical tests and data manipulation. It can also be used to generate fantastic data visualisations. Here we will go through some of the basics of R so that you can better understand the practicals throughout the workshop. Basics To being type R in the terminal: R Libraries Most functionality of R is organised in packages or libraries. To access these functions, we will have to install and load these packages. Most commonly used packages are installed together with the standard installation process. You can install a new library using the install.packages function. For example, to install ggplot2 , run the command: install.packages(\"ggplot2\") After installation, you can load the library by typing library(ggplot2) Variables in R You can assign a value or values to any variable you want using \\<-. e.g Assign a number to a a <- 1 Assign a vector containing a,b,c to v1 v1 <- c(\"a\", \"b\",\"c\") Functions You can perform lots of operations in R using di\ufb00erent built-in R functions. Some examples are below: Assign number of samples nsample <- 10000 Generate nsample random normal variable with mean = 0 and sd = 1 normal <- rnorm(nsample, mean=0,sd=1) normal.2 <- rnorm(nsample, mean=0,sd=1) We can examine the first few entries of the result using head head(normal) And we can obtain the mean and sd using mean(normal) sd(normal) We can also calculate the correlation between two variables using cor cor(normal, normal.2) Plotting While R contains many powerful plotting functions in its base packages,customisationn can be di\ufb03cult (e.g. changing the colour scales, arranging the axes). ggplot2 is a powerful visualization package that provides extensive flexibility and customi- sation of plots. As an example, we can do the following Load the package library(ggplot2) Specify sample size nsample <-1000 Generate random grouping using sample with replacement groups <- sample(c(\"a\",\"b\"), nsample, replace=T) Now generate the data dat <- data.frame(x=rnorm(nsample), y=rnorm(nsample), groups) Generate a scatter plot with di\ufb00erent coloring based on group ggplot(dat, aes(x=x,y=y,color=groups))+geom_point() Regression Models In statistical modelling, regression analyses are a set of statistical techniques for estimating the relationships among variables or features. We can perform regression analysis in R . Use the following code to perform linear regression on simulated variables \"x\" and \"y\": Simulate data nsample <- 10000 x <- rnorm(nsample) y <- rnorm(nsample) Run linear regression lm(y~x) We can store the result into a variable reg <- lm(y~x) And get a detailed output using summary summary(lm(y~x)) We can also extract the coe\ufb03cient of regression using reg$coefficient And we can obtain the residuals by residual <- resid(reg) Examine the first few entries of residuals head(residual) We can also include covariates into the model covar <- rnorm(nsample) lm(y~x+covar) And can even perform interaction analysis lm(y~x+covar+x*covar) Alternatively, we can use the glm function to perform the regression: glm(y~x) For binary traits (case controls studies), logistic regression can be performed using Simulate samples nsample <- 10000 x <- rnorm(nsample) Simulate binary traits (must be coded with 0 and 1) y <- sample(c(0,1), size=nsample, replace=T) Perform logistic regression glm(y~x, family=binomial) Obtain the detailed output summary(glm(y~x, family=binomial))","title":"R Tutorial"},{"location":"tut_R/#introduction-to-r","text":"R is a useful programming language that allows us to perform a variety of statis- tical tests and data manipulation. It can also be used to generate fantastic data visualisations. Here we will go through some of the basics of R so that you can better understand the practicals throughout the workshop.","title":"Introduction to R"},{"location":"tut_R/#basics","text":"To being type R in the terminal: R","title":"Basics"},{"location":"tut_R/#libraries","text":"Most functionality of R is organised in packages or libraries. To access these functions, we will have to install and load these packages. Most commonly used packages are installed together with the standard installation process. You can install a new library using the install.packages function. For example, to install ggplot2 , run the command: install.packages(\"ggplot2\") After installation, you can load the library by typing library(ggplot2)","title":"Libraries"},{"location":"tut_R/#variables-in-r","text":"You can assign a value or values to any variable you want using \\<-. e.g Assign a number to a a <- 1 Assign a vector containing a,b,c to v1 v1 <- c(\"a\", \"b\",\"c\")","title":"Variables in R"},{"location":"tut_R/#functions","text":"You can perform lots of operations in R using di\ufb00erent built-in R functions. Some examples are below: Assign number of samples nsample <- 10000 Generate nsample random normal variable with mean = 0 and sd = 1 normal <- rnorm(nsample, mean=0,sd=1) normal.2 <- rnorm(nsample, mean=0,sd=1) We can examine the first few entries of the result using head head(normal) And we can obtain the mean and sd using mean(normal) sd(normal) We can also calculate the correlation between two variables using cor cor(normal, normal.2)","title":"Functions"},{"location":"tut_R/#plotting","text":"While R contains many powerful plotting functions in its base packages,customisationn can be di\ufb03cult (e.g. changing the colour scales, arranging the axes). ggplot2 is a powerful visualization package that provides extensive flexibility and customi- sation of plots. As an example, we can do the following Load the package library(ggplot2) Specify sample size nsample <-1000 Generate random grouping using sample with replacement groups <- sample(c(\"a\",\"b\"), nsample, replace=T) Now generate the data dat <- data.frame(x=rnorm(nsample), y=rnorm(nsample), groups) Generate a scatter plot with di\ufb00erent coloring based on group ggplot(dat, aes(x=x,y=y,color=groups))+geom_point()","title":"Plotting"},{"location":"tut_R/#regression-models","text":"In statistical modelling, regression analyses are a set of statistical techniques for estimating the relationships among variables or features. We can perform regression analysis in R . Use the following code to perform linear regression on simulated variables \"x\" and \"y\": Simulate data nsample <- 10000 x <- rnorm(nsample) y <- rnorm(nsample) Run linear regression lm(y~x) We can store the result into a variable reg <- lm(y~x) And get a detailed output using summary summary(lm(y~x)) We can also extract the coe\ufb03cient of regression using reg$coefficient And we can obtain the residuals by residual <- resid(reg) Examine the first few entries of residuals head(residual) We can also include covariates into the model covar <- rnorm(nsample) lm(y~x+covar) And can even perform interaction analysis lm(y~x+covar+x*covar) Alternatively, we can use the glm function to perform the regression: glm(y~x) For binary traits (case controls studies), logistic regression can be performed using Simulate samples nsample <- 10000 x <- rnorm(nsample) Simulate binary traits (must be coded with 0 and 1) y <- sample(c(0,1), size=nsample, replace=T) Perform logistic regression glm(y~x, family=binomial) Obtain the detailed output summary(glm(y~x, family=binomial))","title":"Regression Models"},{"location":"tut_bash/","text":"Bash/Shell Tutorial The ability to navigate the filesystem using bash is a very important skill in statistical genetics. Bash is a programming language commonly used to navigate the terminal and manipulate files and folders. Some terminals run \"bash-like\" scripting languages like \"zsh\". For the purposes of this tutorial, they are indistinguishable from bash. There are approximately 50 bash commands that are used 95% of the time . Here we will review some common commands and do some simple file analysis. Common Commands cd This command is used to change our directory, in the following manner: cd $PATH where $PATH represents the path to the target directory. Common usage of cd includes: cd ~/ # will bring you to your home directory cd $HOME # will bring you to your home directory cd ../ # will bring you to the parent directory (up one level) cd Downloads # will bring you to the Downloads directory, provided that you are in the home directory cd ~/Downloads # will bring you to Downloads, no matter where you are in the filesystem ls This command allows you to look at the contents of a directory: ls Some common usage of ls includes: ls # list the contents of the current directory ls ~ # list the contents of the home directory ls ~/Desktop # Will list the contents of the Desktop For ls , there are a number of additional Unix command options that you can append to it to get additional information, for example: ls -l # shows files as list ls -lh # shows files as a list with human readable format ls -lt # shows the files as a list sorted by time-last-edited ls -lS # shows the files as a list sorted by size mkdir This command to create a new directory in your home folder: mkdir ~/test_directory The following commands should be carried out within this directory Use cd to enter the directory: cd ~/test_directory wsl echo echo prints to screen: echo \"Hello\" touch touch creates a new (empty) file: touch foo rm rm deletes a file: rm foo ^ The carrot sign ^ sends the output to a file: echo \"Hello\" > output.txt cat cat prints the entire contents of a file to the screen: cat output.txt less less can be used to view a file: less output.txt to return to the terminal press q cp cp copies a file: cat output.txt output2.txt nano nano can be used to edit a file: nano data1.txt Will bring up an editor window: Starting from the top line type: bob 1 fred 2 mary 3 noah 4 sally 5 And then save the file using Ctrl-O and press enter, and quit using Ctrl-X . wc The word count command wc can be used to count the number of lines or words in a file: wc -l data1.txt grep Can be used to search a file for a string: grep \"noah\" data1.txt will return all the lines in data1.txt containing the string \"noah\". File Analysis A very powerful feature of the terminal is the awk programming language, which allows us to extract subsets of a data file, filter data according to some criteria or perform arithmetic operations on the data. awk manipulates a data file by performing operations on its columns - this is extremely useful for scientific data sets because typically the columns are features or variables of interest. For example, we can use awk to produce a new file that squares the data in our previous file: awk '{print $1,$2*$2}' data1.txt > data2.txt We can also use awk to add up all the squared data values: awk 'BEGIN{total=0} {total+=$2} END{print total}' data2.txt","title":"Bash Tutorial"},{"location":"tut_bash/#bashshell-tutorial","text":"The ability to navigate the filesystem using bash is a very important skill in statistical genetics. Bash is a programming language commonly used to navigate the terminal and manipulate files and folders. Some terminals run \"bash-like\" scripting languages like \"zsh\". For the purposes of this tutorial, they are indistinguishable from bash. There are approximately 50 bash commands that are used 95% of the time . Here we will review some common commands and do some simple file analysis.","title":"Bash/Shell Tutorial"},{"location":"tut_bash/#common-commands","text":"","title":"Common Commands"},{"location":"tut_bash/#cd","text":"This command is used to change our directory, in the following manner: cd $PATH where $PATH represents the path to the target directory. Common usage of cd includes: cd ~/ # will bring you to your home directory cd $HOME # will bring you to your home directory cd ../ # will bring you to the parent directory (up one level) cd Downloads # will bring you to the Downloads directory, provided that you are in the home directory cd ~/Downloads # will bring you to Downloads, no matter where you are in the filesystem","title":"cd"},{"location":"tut_bash/#ls","text":"This command allows you to look at the contents of a directory: ls Some common usage of ls includes: ls # list the contents of the current directory ls ~ # list the contents of the home directory ls ~/Desktop # Will list the contents of the Desktop For ls , there are a number of additional Unix command options that you can append to it to get additional information, for example: ls -l # shows files as list ls -lh # shows files as a list with human readable format ls -lt # shows the files as a list sorted by time-last-edited ls -lS # shows the files as a list sorted by size","title":"ls"},{"location":"tut_bash/#mkdir","text":"This command to create a new directory in your home folder: mkdir ~/test_directory The following commands should be carried out within this directory Use cd to enter the directory: cd ~/test_directory wsl","title":"mkdir"},{"location":"tut_bash/#echo","text":"echo prints to screen: echo \"Hello\"","title":"echo"},{"location":"tut_bash/#touch","text":"touch creates a new (empty) file: touch foo","title":"touch"},{"location":"tut_bash/#rm","text":"rm deletes a file: rm foo","title":"rm"},{"location":"tut_bash/#_1","text":"The carrot sign ^ sends the output to a file: echo \"Hello\" > output.txt","title":"^"},{"location":"tut_bash/#cat","text":"cat prints the entire contents of a file to the screen: cat output.txt","title":"cat"},{"location":"tut_bash/#less","text":"less can be used to view a file: less output.txt to return to the terminal press q","title":"less"},{"location":"tut_bash/#cp","text":"cp copies a file: cat output.txt output2.txt","title":"cp"},{"location":"tut_bash/#nano","text":"nano can be used to edit a file: nano data1.txt Will bring up an editor window: Starting from the top line type: bob 1 fred 2 mary 3 noah 4 sally 5 And then save the file using Ctrl-O and press enter, and quit using Ctrl-X .","title":"nano"},{"location":"tut_bash/#wc","text":"The word count command wc can be used to count the number of lines or words in a file: wc -l data1.txt","title":"wc"},{"location":"tut_bash/#grep","text":"Can be used to search a file for a string: grep \"noah\" data1.txt will return all the lines in data1.txt containing the string \"noah\".","title":"grep"},{"location":"tut_bash/#file-analysis","text":"A very powerful feature of the terminal is the awk programming language, which allows us to extract subsets of a data file, filter data according to some criteria or perform arithmetic operations on the data. awk manipulates a data file by performing operations on its columns - this is extremely useful for scientific data sets because typically the columns are features or variables of interest. For example, we can use awk to produce a new file that squares the data in our previous file: awk '{print $1,$2*$2}' data1.txt > data2.txt We can also use awk to add up all the squared data values: awk 'BEGIN{total=0} {total+=$2} END{print total}' data2.txt","title":"File Analysis"},{"location":"tut_intro/","text":"Introduction Here we present pre-workshop tutorials for bash/shell , R , and Python3 , and Plink . Students who are not already very familiar with these programs are required to complete these tutorials. The first three tutorials do not require any data and can be completed at anytime while the last tutorial (Plink) requires sample data downloaded in Testing . When the workshop begins students will be expected to understand the following: Bash : Navigate filesystem, copy/move/unzip files. Create directories and edit files (using nano). R : Install packages, read from file, manipulate variables, understand functions, create plots. Python : Import libraries, create variables, understand list comprehension, basic plotting. Plink : Explore and genderate GWAS data, recode and reorder allelic data, use PLINK website.","title":"Introduction"},{"location":"tut_intro/#introduction","text":"Here we present pre-workshop tutorials for bash/shell , R , and Python3 , and Plink . Students who are not already very familiar with these programs are required to complete these tutorials. The first three tutorials do not require any data and can be completed at anytime while the last tutorial (Plink) requires sample data downloaded in Testing . When the workshop begins students will be expected to understand the following: Bash : Navigate filesystem, copy/move/unzip files. Create directories and edit files (using nano). R : Install packages, read from file, manipulate variables, understand functions, create plots. Python : Import libraries, create variables, understand list comprehension, basic plotting. Plink : Explore and genderate GWAS data, recode and reorder allelic data, use PLINK website.","title":"Introduction"},{"location":"tut_plink/","text":"Introduction to PLINK (Part I) PLINK is the most popular software program for performing genome-wide association analyses, it is extremely extensive allowing a huge number of analyses to be performed. It also includes many options for reformatting your data and provides useful data summaries. Software packages are usually best learnt by having a go at running some of their basic applications and progressing from there (rather than reading the entire user manual first!) - so we begin by running some basic PLINK commands and then work steadily towards performing more sophisticated analyses through these PLINK tutorials. Sample Data This tutorial runs on the data in the pre-workshop materials downloaded here . If you have followed the previous directions, this data should be in: ~/prsworkshop/preworkshop_materials_(\"your OS\")/Plink/tutorial/sample_data . \u26a0\ufe0f All data used in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only. After completing this practical, you should be able to: Explore and generate genetic data sets needed for GWAS Recode and reorder allelic data Use the PLINK website Select and exclude lists of samples and SNPs In all of the instructions below: - Anything in between the symbols \\<> needs to be changed in some way. For example, \\<file_name> indicates that you should replace that entire statement (including the \\<> symbols) with the appropriate file name. - Bold indicates non- command-line instructions (e.g. right-click ) Exploring Data To begin the tutorial please navigate to: cd ~/prsworkshop/preworkshop_materials_(\"yourOS\")/Plink/tutorial/sample_data First let's make sure we can call plink from this directory: ../../code/plink Next lets observe the files in the sample data directory: ls We should see the following four files: D1D.ped, D1D.map, D1D.pcs1234, D1D.pheno1 . We first convert the \"old\" format ped/map files to the more memory efficient binary format using the following command: ../../code/plink --file D1D --make-bed --out D1D This generates three new files, D1D.bim, D1D.fam, D1D.bed . Type ls -l , compare how much disk space the bim/fam/bed and ped/map files use. Let's look at the following files by typing the following commands and pressing q to quit after each one: less D1D.bim # Marker / SNP information less D1D.fam # Individual information: IDs less D1D.pcs1234 # A PCA file that lists individuals first four prinicipal components less D1D.pheno1 # A phenotype file that lists individuals phenotypes D1D.bed is a binary file and stores the genotype do not open this file . Investigate the format of the bim and fam files here https://zzz.bwh.harvard.edu/plink/data.shtml#bed, scroll up for details. What do you observe? - What are columns 1, 2, 4, 5, 6 of the bim file? - What are the columns of the fam file? Recoding alleles as counts Genotype data in allele count format is very useful, for example to use in regression modelling in statistical software such as R. Generate the D1D data in allele count format: ../../code/plink --bfile D1D --recodeA --out D1D_AC \ud83d\udcdd There are several options for recoding SNPs in different ways - more information on the PLINK website (see next section). Again note that a log file was created - skim the log file or screen output Look inside the .raw file. What do you think the 0/1/2 represent? Do there appear to be more 0s or 2s? Why might this be? PLINK website Go to the plink website and skim through the front page to get an idea of PLINK's functionality. Note the list of clickable links on the left side of the website. Under 'Data Management' (click the heading on the left) and read the list of the di\ufb00erent ways you may want to recode and reorder data sets. Don't attempt to read much further as this is a very large and detailed section - a useful future resource but too much for today. Under 'Input filtering', read the different ways SNPs can be filtered. Write SNP list and extract SNPs The --write-snplist writes a list of SNPs (penultimate argument in 'Data Management'). Use this command along with the information that you found on the PLINK website to create a command to extract a list of SNPs. Below is a list of requirements - try to do this before you go to the end of this section, where the full command is given and explained. Set the D1D binary file as input Set MAF threshold to 0.05 Set SNP missingness threshold to 0.05 Add the appropriate command to write out a snp list containing only those SNPs with MAF above 0.05 and missingness below 0.05 Use 'D1D_snps' as the output file name After the command has run, check the output for your SNP list and look at it with the default viewer. You will now use the SNP list that you have created to extract those SNPs and create a new set of data files in a single command. Use the D1D binary file set as input Find the command for extracting a set of SNPs listed in a file (hint: Data Management section) and combine it with a command that you learned above to create binary files Use the output file name 'D1D_MAF_MISS' \ud83d\udcdd Log files are uselful to check that the number of SNPs and samples is as expected. Always check your your log files to ensure that they are sensible. SNP lists can also be used to EXCLUDE SNPs - select 'exclude' above instead of 'extract'. Sample ID lists can also be used to 'keep' or 'remove' individuals in the same 'filter' window. Note that both sample IDs (FID IID,separated by a space are required in the sample file list. Solution 1: TO BE REVEALED LATER!! Solution 2: TO BE REVEALED LATER!! Performing QC & GWAS (Part II) Here we will work on the following skills: Generate summaries of the data needed for QC Apply QC thresholds Perform GWAS Generate summaries to perform QC There are many kinds of summaries of the data that can be generated by PLINK in order to perform particular quality control (QC) steps, which help to make our data more reliable. Some of these involve summaries in relation to the individuals (e.g. individual missingness, sex-check) and some relate to summaries of SNP data (e.g. MAF, Hardy-Weinburg Equilibrium). Over the next few sub-sections you will go through some examples of generating summary statistics that can be used to perform QC. Individual missingness Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Open the 2 files that were generated (lmiss & imiss). What do the two output files contain? In the imiss file, what is the meaning of the data in the column headed \"F_MISS\"? SNP Missingness Look inside the file containing SNP missingness information: D1D_miss.lmiss. What is the meaning of the value under F_MISS? What does the command --test-missing do and why might it be useful? Hardy-Weinberg Equilibrium Generate HWE statistics using the --hardy option. Use output file name D1D_hardy. Open and examine results. Why are there multiple rows for each SNP and what does each mean? Which of the rows do you think should be used to exclude SNPs from the subsequent analysis (if any) for failing the HWE test? Why? Allele frequencies Generate allele frequencies using the command *--*freq. Use D1D_freq as the output name. Examine the output. What is the heading of the column that tells you which nucleotide is the minor allele? \ud83d\udcdd This information is important to remember as many PLINK files use this notation. The minor allele is always labeled the same way Apply QC filters There are di\ufb00erent strategies for performing QC on your data: (a) Create lists of SNPs and individuals and use --remove, --extract, --exclude, --include to create new file sets (good for documentation, collaboration) (b) Apply thresholds one at a time and generate new bed/bim/fam file (good for applying sequential filters) (c) Use options (e.g. --maf ) in other commands (e.g. --assoc) to remove SNPs or samples at required QC thresholds during analysis. \ud83d\udcdd We have already seen how to select or exclude individuals or SNPs by first creating lists (a), so in this section we will set thresholds to generate new files sets in a single command. However, it is useful to have lists of all SNPs and individuals excluded pre-analysis, according to the reason for exclusion, so generating and retaining such files using the techniques that we used before for good practice. Apply individual missingness thresholds Generate new binary file sets (--make-bed) from the 'D1D' binary file set, removing individuals with missingness greater than 3% using a single command (hint: In the 'Inclusion thresholds' section, see the 'Missing/person' sub-section). Use the output file name 'D1D_imiss3pc' Examine the output files (no need to open, and remember the bed file cannot be read) and the log file How many individuals were in the original file? How many individuals were removed? How many males and females were left after screening? Apply SNP missingness and MAF thresholds Create new binary file sets from the 'D1D_imiss3pc' binary file set (NOT the original D1D files) by setting MAF threshold to 0.05 and SNP missingness threshold to 0.02 (See 'Inclusion thresholds' to obtain the correct threshold flags). Use the output file name'D1D_imiss3pc_lmiss2pc_maf5pc Examine the output files and the log file How many SNPs were in the original files? How many SNPs were removed for low minor allele frequency? How many SNPs were removed for missingness? Apply Hardy-Weinberg thresholds Generate a new binary file set called 'D1D_QC' from the D1D_imiss3pc_lmiss2pc_maf5pc file, applying a HWE threshold of 0.0001. This is our final, QC'ed file set. Examine log and output files. -How many SNPs were removed for HWE p-values below the threshold? \ud83d\udcdd It is useful to know how to do this, but be careful about setting this threshold - strong association signals can cause departure from HWE and you may remove great results! Use a lenient threshold and apply to controls only to avoid this problem. HWE can also be checked post-hoc for each SNP. Perform GWAS Case/Control GWAS - no covariates Run the following code, which performs a genetic association study using logistic regression on some case/control data: ../../code/plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --out D1D_CC What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Are there any other significant associations? Case/Control GWAS - with covariates Here we repeat the previous analysis but this time including some covariates. The file D1D.pcs1234 contains the first 4 principal components from a PCA on the genetic data. Run the analysis specifying the covariates file: ../../code/plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --covar D1D.pcs1234 --out D1D_CC_PCadj What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Suggest a reason for the different results when adjusting for the 4 PCs?","title":"Introduction to PLINK  (Part I)"},{"location":"tut_plink/#introduction-to-plink-part-i","text":"PLINK is the most popular software program for performing genome-wide association analyses, it is extremely extensive allowing a huge number of analyses to be performed. It also includes many options for reformatting your data and provides useful data summaries. Software packages are usually best learnt by having a go at running some of their basic applications and progressing from there (rather than reading the entire user manual first!) - so we begin by running some basic PLINK commands and then work steadily towards performing more sophisticated analyses through these PLINK tutorials.","title":"Introduction to PLINK  (Part I)"},{"location":"tut_plink/#sample-data","text":"This tutorial runs on the data in the pre-workshop materials downloaded here . If you have followed the previous directions, this data should be in: ~/prsworkshop/preworkshop_materials_(\"your OS\")/Plink/tutorial/sample_data . \u26a0\ufe0f All data used in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only. After completing this practical, you should be able to: Explore and generate genetic data sets needed for GWAS Recode and reorder allelic data Use the PLINK website Select and exclude lists of samples and SNPs In all of the instructions below: - Anything in between the symbols \\<> needs to be changed in some way. For example, \\<file_name> indicates that you should replace that entire statement (including the \\<> symbols) with the appropriate file name. - Bold indicates non- command-line instructions (e.g. right-click )","title":"Sample Data"},{"location":"tut_plink/#exploring-data","text":"To begin the tutorial please navigate to: cd ~/prsworkshop/preworkshop_materials_(\"yourOS\")/Plink/tutorial/sample_data First let's make sure we can call plink from this directory: ../../code/plink Next lets observe the files in the sample data directory: ls We should see the following four files: D1D.ped, D1D.map, D1D.pcs1234, D1D.pheno1 . We first convert the \"old\" format ped/map files to the more memory efficient binary format using the following command: ../../code/plink --file D1D --make-bed --out D1D This generates three new files, D1D.bim, D1D.fam, D1D.bed . Type ls -l , compare how much disk space the bim/fam/bed and ped/map files use. Let's look at the following files by typing the following commands and pressing q to quit after each one: less D1D.bim # Marker / SNP information less D1D.fam # Individual information: IDs less D1D.pcs1234 # A PCA file that lists individuals first four prinicipal components less D1D.pheno1 # A phenotype file that lists individuals phenotypes D1D.bed is a binary file and stores the genotype do not open this file . Investigate the format of the bim and fam files here https://zzz.bwh.harvard.edu/plink/data.shtml#bed, scroll up for details. What do you observe? - What are columns 1, 2, 4, 5, 6 of the bim file? - What are the columns of the fam file?","title":"Exploring Data"},{"location":"tut_plink/#recoding-alleles-as-counts","text":"Genotype data in allele count format is very useful, for example to use in regression modelling in statistical software such as R. Generate the D1D data in allele count format: ../../code/plink --bfile D1D --recodeA --out D1D_AC \ud83d\udcdd There are several options for recoding SNPs in different ways - more information on the PLINK website (see next section). Again note that a log file was created - skim the log file or screen output Look inside the .raw file. What do you think the 0/1/2 represent? Do there appear to be more 0s or 2s? Why might this be?","title":"Recoding alleles as counts"},{"location":"tut_plink/#plink-website","text":"Go to the plink website and skim through the front page to get an idea of PLINK's functionality. Note the list of clickable links on the left side of the website. Under 'Data Management' (click the heading on the left) and read the list of the di\ufb00erent ways you may want to recode and reorder data sets. Don't attempt to read much further as this is a very large and detailed section - a useful future resource but too much for today. Under 'Input filtering', read the different ways SNPs can be filtered.","title":"PLINK website"},{"location":"tut_plink/#write-snp-list-and-extract-snps","text":"The --write-snplist writes a list of SNPs (penultimate argument in 'Data Management'). Use this command along with the information that you found on the PLINK website to create a command to extract a list of SNPs. Below is a list of requirements - try to do this before you go to the end of this section, where the full command is given and explained. Set the D1D binary file as input Set MAF threshold to 0.05 Set SNP missingness threshold to 0.05 Add the appropriate command to write out a snp list containing only those SNPs with MAF above 0.05 and missingness below 0.05 Use 'D1D_snps' as the output file name After the command has run, check the output for your SNP list and look at it with the default viewer. You will now use the SNP list that you have created to extract those SNPs and create a new set of data files in a single command. Use the D1D binary file set as input Find the command for extracting a set of SNPs listed in a file (hint: Data Management section) and combine it with a command that you learned above to create binary files Use the output file name 'D1D_MAF_MISS' \ud83d\udcdd Log files are uselful to check that the number of SNPs and samples is as expected. Always check your your log files to ensure that they are sensible. SNP lists can also be used to EXCLUDE SNPs - select 'exclude' above instead of 'extract'. Sample ID lists can also be used to 'keep' or 'remove' individuals in the same 'filter' window. Note that both sample IDs (FID IID,separated by a space are required in the sample file list. Solution 1: TO BE REVEALED LATER!! Solution 2: TO BE REVEALED LATER!!","title":"Write SNP list and extract SNPs"},{"location":"tut_plink/#performing-qc-gwas-part-ii","text":"Here we will work on the following skills: Generate summaries of the data needed for QC Apply QC thresholds Perform GWAS","title":"Performing QC &amp; GWAS (Part II)"},{"location":"tut_plink/#generate-summaries-to-perform-qc","text":"There are many kinds of summaries of the data that can be generated by PLINK in order to perform particular quality control (QC) steps, which help to make our data more reliable. Some of these involve summaries in relation to the individuals (e.g. individual missingness, sex-check) and some relate to summaries of SNP data (e.g. MAF, Hardy-Weinburg Equilibrium). Over the next few sub-sections you will go through some examples of generating summary statistics that can be used to perform QC.","title":"Generate summaries to perform QC"},{"location":"tut_plink/#individual-missingness","text":"Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Open the 2 files that were generated (lmiss & imiss). What do the two output files contain? In the imiss file, what is the meaning of the data in the column headed \"F_MISS\"?","title":"Individual missingness"},{"location":"tut_plink/#snp-missingness","text":"Look inside the file containing SNP missingness information: D1D_miss.lmiss. What is the meaning of the value under F_MISS? What does the command --test-missing do and why might it be useful?","title":"SNP Missingness"},{"location":"tut_plink/#hardy-weinberg-equilibrium","text":"Generate HWE statistics using the --hardy option. Use output file name D1D_hardy. Open and examine results. Why are there multiple rows for each SNP and what does each mean? Which of the rows do you think should be used to exclude SNPs from the subsequent analysis (if any) for failing the HWE test? Why?","title":"Hardy-Weinberg Equilibrium"},{"location":"tut_plink/#allele-frequencies","text":"Generate allele frequencies using the command *--*freq. Use D1D_freq as the output name. Examine the output. What is the heading of the column that tells you which nucleotide is the minor allele? \ud83d\udcdd This information is important to remember as many PLINK files use this notation. The minor allele is always labeled the same way","title":"Allele frequencies"},{"location":"tut_plink/#apply-qc-filters","text":"","title":"Apply QC filters"},{"location":"tut_plink/#there-are-different-strategies-for-performing-qc-on-your-data","text":"(a) Create lists of SNPs and individuals and use --remove, --extract, --exclude, --include to create new file sets (good for documentation, collaboration) (b) Apply thresholds one at a time and generate new bed/bim/fam file (good for applying sequential filters) (c) Use options (e.g. --maf ) in other commands (e.g. --assoc) to remove SNPs or samples at required QC thresholds during analysis. \ud83d\udcdd We have already seen how to select or exclude individuals or SNPs by first creating lists (a), so in this section we will set thresholds to generate new files sets in a single command. However, it is useful to have lists of all SNPs and individuals excluded pre-analysis, according to the reason for exclusion, so generating and retaining such files using the techniques that we used before for good practice.","title":"There are di\ufb00erent strategies for performing QC on your data:"},{"location":"tut_plink/#apply-individual-missingness-thresholds","text":"Generate new binary file sets (--make-bed) from the 'D1D' binary file set, removing individuals with missingness greater than 3% using a single command (hint: In the 'Inclusion thresholds' section, see the 'Missing/person' sub-section). Use the output file name 'D1D_imiss3pc' Examine the output files (no need to open, and remember the bed file cannot be read) and the log file How many individuals were in the original file? How many individuals were removed? How many males and females were left after screening?","title":"Apply individual missingness thresholds"},{"location":"tut_plink/#apply-snp-missingness-and-maf-thresholds","text":"Create new binary file sets from the 'D1D_imiss3pc' binary file set (NOT the original D1D files) by setting MAF threshold to 0.05 and SNP missingness threshold to 0.02 (See 'Inclusion thresholds' to obtain the correct threshold flags). Use the output file name'D1D_imiss3pc_lmiss2pc_maf5pc Examine the output files and the log file How many SNPs were in the original files? How many SNPs were removed for low minor allele frequency? How many SNPs were removed for missingness?","title":"Apply SNP missingness and MAF thresholds"},{"location":"tut_plink/#apply-hardy-weinberg-thresholds","text":"Generate a new binary file set called 'D1D_QC' from the D1D_imiss3pc_lmiss2pc_maf5pc file, applying a HWE threshold of 0.0001. This is our final, QC'ed file set. Examine log and output files. -How many SNPs were removed for HWE p-values below the threshold? \ud83d\udcdd It is useful to know how to do this, but be careful about setting this threshold - strong association signals can cause departure from HWE and you may remove great results! Use a lenient threshold and apply to controls only to avoid this problem. HWE can also be checked post-hoc for each SNP.","title":"Apply Hardy-Weinberg thresholds"},{"location":"tut_plink/#perform-gwas","text":"","title":"Perform GWAS"},{"location":"tut_plink/#casecontrol-gwas-no-covariates","text":"Run the following code, which performs a genetic association study using logistic regression on some case/control data: ../../code/plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --out D1D_CC What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Are there any other significant associations?","title":"Case/Control GWAS - no covariates"},{"location":"tut_plink/#casecontrol-gwas-with-covariates","text":"Here we repeat the previous analysis but this time including some covariates. The file D1D.pcs1234 contains the first 4 principal components from a PCA on the genetic data. Run the analysis specifying the covariates file: ../../code/plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --covar D1D.pcs1234 --out D1D_CC_PCadj What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Suggest a reason for the different results when adjusting for the 4 PCs?","title":"Case/Control GWAS - with covariates"},{"location":"tut_python/","text":"Introduction to Python Python is a useful general purpose programming language. Here we will go over some basic python commands from an interactive shell. To being open python3 by typing: python3 Libraries Downloaded python libraries can be imported interactively: import matplotlib.pyplot as plt Variables in Python You can assign a value or values to any variable you want using the equals sign: X = 5 # X is an integer (5) X = [1,2,3,4,5] # X is a list containing the first five integers Functions can also be used: X = range(-10,11,1) # X includes all integers between -10 and 10 counting by 1 New variables can be declared with lists: Y = [x*x for x in X] # Y is equal to X squared A figure can be generated: plt.plot(X,Y) And viewed: plt.show()","title":"Python Tutorial"},{"location":"tut_python/#introduction-to-python","text":"Python is a useful general purpose programming language. Here we will go over some basic python commands from an interactive shell. To being open python3 by typing: python3","title":"Introduction to Python"},{"location":"tut_python/#libraries","text":"Downloaded python libraries can be imported interactively: import matplotlib.pyplot as plt","title":"Libraries"},{"location":"tut_python/#variables-in-python","text":"You can assign a value or values to any variable you want using the equals sign: X = 5 # X is an integer (5) X = [1,2,3,4,5] # X is a list containing the first five integers Functions can also be used: X = range(-10,11,1) # X includes all integers between -10 and 10 counting by 1 New variables can be declared with lists: Y = [x*x for x in X] # Y is equal to X squared A figure can be generated: plt.plot(X,Y) And viewed: plt.show()","title":"Variables in Python"},{"location":"modules/module1/","text":"What is Machine Learning Table of contents Key learning outcomes ML/LLM Basic Relevant Biostatistcs Linux: Learn it, Love it ML/LLM Training - Image Analysis Key learning outcomes In this module we will focus on the basics - how to use linux, basic relevant biostatistical theory (data, statistical approaches, etc), and what machine learning is (and importantly, is not) conceptually. ML/LLM: The Basics To be added. Questions? To be added Conceptual Exercise: PCA Principle component analysis is a useful technique that allows researchers to visualize high dimensional data in lower space by rotating the axes in such a way that the lower dimensions (or components) maximize the total variance explained. In statistical genetics this involves \"rotating\" million-dimensional data - something that is very hard to visualize! For this reason, we begin with a simpler exercise. For the following three two dimensional shapes, spend some time identifying the principle components or sketching the line across for which variance is maximized. Check your answers below: \u2753QUESTIONS: What line represents the principle component for the first shape? What line represents the principle component for the second shape? What line represents the principle component for the third shape? Below you can view the shapes in principal component space. Now that we understand how PCA works in two dimensions we will consider a higher dimensional example. In the three dimensional space below, see if you can visualize a plane that maximizes the variance across two dimensions: Did you get it right? If so, realize that this is equivalent to what we do in genetics - we find rotate the data through millions of dimensions of space to find the plane that maximizes the variance in two dimensions: To run PCA with real data please enter the exercise3 directory, and type the following command to run PCA on the 1000 Genome data: ./code/plink --bfile data/chr1-22 --indep-pairwise 250 25 0.1 --maf 0.1 --threads 30 --out chr1-22.ldpruned_all_1kgv2 ./code/plink --bfile data/chr1-22 --extract chr1-22.ldpruned_all_1kgv2.prune.in --pca --threads 30 This will generate the principal components that maximize the variance in the data. To plot the result run the following commands from with an R-terminal: R-Code: Generate a PCA Plot require('RColorBrewer') options(scipen=100, digits=3) eigenvec <- read.table('plink.eigenvec', header = F, skip=0, sep = ' ') rownames(eigenvec) <- eigenvec[,2] eigenvec <- eigenvec[,3:ncol(eigenvec)] colnames(eigenvec) <- paste('Principal Component ', c(1:20), sep = '') PED <- read.table(\"data/all_phase3.king.psam\", header = TRUE, skip = 0, sep = '\\t') PED <- PED[which(PED$IID %in% rownames(eigenvec)), ] PED <- PED[match(rownames(eigenvec), PED$IID),] PED$Population <- factor(PED$Population, levels=c(\"ACB\",\"ASW\",\"ESN\",\"GWD\",\"LWK\",\"MSL\",\"YRI\",\"CLM\",\"MXL\",\"PEL\",\"PUR\",\"CDX\",\"CHB\",\"CHS\",\"JPT\",\"KHV\",\"CEU\",\"FIN\",\"GBR\",\"IBS\",\"TSI\",\"BEB\",\"GIH\",\"ITU\",\"PJL\",\"STU\")) col <- colorRampPalette(c(\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"forestgreen\",\"forestgreen\",\"forestgreen\",\"forestgreen\",\"grey\",\"grey\",\"grey\",\"grey\",\"grey\", \"royalblue\",\"royalblue\",\"royalblue\",\"royalblue\",\"royalblue\",\"black\",\"black\",\"black\",\"black\",\"black\"))(length(unique(PED$Population)))[factor(PED$Population)] project.pca <- eigenvec par(mar = c(5,5,5,5), cex = 2.0,cex.main = 7, cex.axis = 2.75, cex.lab = 2.75, mfrow = c(1,2)) plot(project.pca[,1], project.pca[,2], type = 'n', main = 'A', adj = 0.5, xlab = 'First component', ylab = 'Second component', font = 2, font.lab = 2) points(project.pca[,1], project.pca[,2], col = col, pch = 20, cex = 2.25) legend('bottomright', bty = 'n', cex = 3.0, title = '', c('AFR', 'AMR', 'EAS', 'EUR', 'SAS'), fill = c('yellow', 'forestgreen', 'grey', 'royalblue', 'black')) plot(project.pca[,1], project.pca[,3], type=\"n\", main=\"B\", adj=0.5, xlab=\"First component\", ylab=\"Third component\", font=2, font.lab=2) points(project.pca[,1], project.pca[,3], col=col, pch=20, cex=2.25) \u2753QUESTIONS: What is distinct about the PC projections of the AMR group relative to other populations? Why does this occur? What does it tell us about ancestry of this group? Back to table of contents Basic Biostats To be added Questions? To be added Back to table of contents Linux Primer To be added Questions? To be added Back to table of contents ML/LLM Training To be added Questions? To be added Back to table of contents","title":"Module1"},{"location":"modules/module1/#what-is-machine-learning","text":"","title":"What is Machine Learning"},{"location":"modules/module1/#table-of-contents","text":"Key learning outcomes ML/LLM Basic Relevant Biostatistcs Linux: Learn it, Love it ML/LLM Training - Image Analysis","title":"Table of contents"},{"location":"modules/module1/#key-learning-outcomes","text":"In this module we will focus on the basics - how to use linux, basic relevant biostatistical theory (data, statistical approaches, etc), and what machine learning is (and importantly, is not) conceptually.","title":"Key learning outcomes"},{"location":"modules/module1/#mlllm-the-basics","text":"To be added.","title":"ML/LLM: The Basics"},{"location":"modules/module1/#questions","text":"To be added","title":"Questions?"},{"location":"modules/module1/#conceptual-exercise-pca","text":"Principle component analysis is a useful technique that allows researchers to visualize high dimensional data in lower space by rotating the axes in such a way that the lower dimensions (or components) maximize the total variance explained. In statistical genetics this involves \"rotating\" million-dimensional data - something that is very hard to visualize! For this reason, we begin with a simpler exercise. For the following three two dimensional shapes, spend some time identifying the principle components or sketching the line across for which variance is maximized. Check your answers below: \u2753QUESTIONS: What line represents the principle component for the first shape? What line represents the principle component for the second shape? What line represents the principle component for the third shape? Below you can view the shapes in principal component space. Now that we understand how PCA works in two dimensions we will consider a higher dimensional example. In the three dimensional space below, see if you can visualize a plane that maximizes the variance across two dimensions: Did you get it right? If so, realize that this is equivalent to what we do in genetics - we find rotate the data through millions of dimensions of space to find the plane that maximizes the variance in two dimensions: To run PCA with real data please enter the exercise3 directory, and type the following command to run PCA on the 1000 Genome data: ./code/plink --bfile data/chr1-22 --indep-pairwise 250 25 0.1 --maf 0.1 --threads 30 --out chr1-22.ldpruned_all_1kgv2 ./code/plink --bfile data/chr1-22 --extract chr1-22.ldpruned_all_1kgv2.prune.in --pca --threads 30 This will generate the principal components that maximize the variance in the data. To plot the result run the following commands from with an R-terminal: R-Code: Generate a PCA Plot require('RColorBrewer') options(scipen=100, digits=3) eigenvec <- read.table('plink.eigenvec', header = F, skip=0, sep = ' ') rownames(eigenvec) <- eigenvec[,2] eigenvec <- eigenvec[,3:ncol(eigenvec)] colnames(eigenvec) <- paste('Principal Component ', c(1:20), sep = '') PED <- read.table(\"data/all_phase3.king.psam\", header = TRUE, skip = 0, sep = '\\t') PED <- PED[which(PED$IID %in% rownames(eigenvec)), ] PED <- PED[match(rownames(eigenvec), PED$IID),] PED$Population <- factor(PED$Population, levels=c(\"ACB\",\"ASW\",\"ESN\",\"GWD\",\"LWK\",\"MSL\",\"YRI\",\"CLM\",\"MXL\",\"PEL\",\"PUR\",\"CDX\",\"CHB\",\"CHS\",\"JPT\",\"KHV\",\"CEU\",\"FIN\",\"GBR\",\"IBS\",\"TSI\",\"BEB\",\"GIH\",\"ITU\",\"PJL\",\"STU\")) col <- colorRampPalette(c(\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"forestgreen\",\"forestgreen\",\"forestgreen\",\"forestgreen\",\"grey\",\"grey\",\"grey\",\"grey\",\"grey\", \"royalblue\",\"royalblue\",\"royalblue\",\"royalblue\",\"royalblue\",\"black\",\"black\",\"black\",\"black\",\"black\"))(length(unique(PED$Population)))[factor(PED$Population)] project.pca <- eigenvec par(mar = c(5,5,5,5), cex = 2.0,cex.main = 7, cex.axis = 2.75, cex.lab = 2.75, mfrow = c(1,2)) plot(project.pca[,1], project.pca[,2], type = 'n', main = 'A', adj = 0.5, xlab = 'First component', ylab = 'Second component', font = 2, font.lab = 2) points(project.pca[,1], project.pca[,2], col = col, pch = 20, cex = 2.25) legend('bottomright', bty = 'n', cex = 3.0, title = '', c('AFR', 'AMR', 'EAS', 'EUR', 'SAS'), fill = c('yellow', 'forestgreen', 'grey', 'royalblue', 'black')) plot(project.pca[,1], project.pca[,3], type=\"n\", main=\"B\", adj=0.5, xlab=\"First component\", ylab=\"Third component\", font=2, font.lab=2) points(project.pca[,1], project.pca[,3], col=col, pch=20, cex=2.25) \u2753QUESTIONS: What is distinct about the PC projections of the AMR group relative to other populations? Why does this occur? What does it tell us about ancestry of this group? Back to table of contents","title":"Conceptual Exercise: PCA"},{"location":"modules/module1/#basic-biostats","text":"To be added","title":"Basic Biostats"},{"location":"modules/module1/#questions_1","text":"To be added Back to table of contents","title":"Questions?"},{"location":"modules/module1/#linux-primer","text":"To be added","title":"Linux Primer"},{"location":"modules/module1/#questions_2","text":"To be added Back to table of contents","title":"Questions?"},{"location":"modules/module1/#mlllm-training","text":"To be added","title":"ML/LLM Training"},{"location":"modules/module1/#questions_3","text":"To be added Back to table of contents","title":"Questions?"},{"location":"modules/module1_backup/","text":"What is Machine Learning Table of contents Key learning outcomes ML/LLM Basic Relevant Biostatistcs Linux: Learn it, Love it ML/LLM Training - Image Analysis Key learning outcomes In this module we will focus on the basics - how to use linux, basic relevant biostatistical theory (data, statistical approaches, etc), and what machine learning is (and importantly, is not) conceptually. ML/LLM: The Basics To be added. Questions? To be added Basic Biostats To be added Questions? To be added Back to table of contents Linux Primer To be added Questions? To be added Back to table of contents ML/LLM Training To be added Questions? To be added Back to table of contents","title":"What is Machine Learning"},{"location":"modules/module1_backup/#what-is-machine-learning","text":"","title":"What is Machine Learning"},{"location":"modules/module1_backup/#table-of-contents","text":"Key learning outcomes ML/LLM Basic Relevant Biostatistcs Linux: Learn it, Love it ML/LLM Training - Image Analysis","title":"Table of contents"},{"location":"modules/module1_backup/#key-learning-outcomes","text":"In this module we will focus on the basics - how to use linux, basic relevant biostatistical theory (data, statistical approaches, etc), and what machine learning is (and importantly, is not) conceptually.","title":"Key learning outcomes"},{"location":"modules/module1_backup/#mlllm-the-basics","text":"To be added.","title":"ML/LLM: The Basics"},{"location":"modules/module1_backup/#questions","text":"To be added","title":"Questions?"},{"location":"modules/module1_backup/#basic-biostats","text":"To be added","title":"Basic Biostats"},{"location":"modules/module1_backup/#questions_1","text":"To be added Back to table of contents","title":"Questions?"},{"location":"modules/module1_backup/#linux-primer","text":"To be added","title":"Linux Primer"},{"location":"modules/module1_backup/#questions_2","text":"To be added Back to table of contents","title":"Questions?"},{"location":"modules/module1_backup/#mlllm-training","text":"To be added","title":"ML/LLM Training"},{"location":"modules/module1_backup/#questions_3","text":"To be added Back to table of contents","title":"Questions?"},{"location":"modules/module2/","text":"More on Machine Learning Table of contents Key learning outcomes LLMs and Research? Uncategorized Data LLMs and the Clinic Module 2: Machine Learning: what is it good for? class sessions re: limitations and liability in LLM, applications in research Monday 7/21: LLMs and Research: Overview Liability and LLM (Ideally guest lawyer, if not, Tom) Thursday 7/24: Liability and LLM (Ideally guest lawyer, if not, Tom) Monday 7/28: Uncategorized Data: the Final Frontier (Tade, Olumide, Tom) Thursday 7/31: LLM and clinical practice: ups and downs. Key learning outcomes In this module we will get deeper into machine learning. We will focus on limitations and liability in LLM, applications in research, (uncategorized data, image analysis, etc), applications in clinical practice (communications/etc), and places where you should NOT use LLM. LLMS and Research To be added. Questions? To be added Uncategorized Data To be added Questions? To be added Back to table of contents LLMs in the Clinic To be added Questions? To be added Back to table of contents","title":"Module2"},{"location":"modules/module2/#more-on-machine-learning","text":"","title":"More on Machine Learning"},{"location":"modules/module2/#table-of-contents","text":"Key learning outcomes LLMs and Research? Uncategorized Data LLMs and the Clinic Module 2: Machine Learning: what is it good for? class sessions re: limitations and liability in LLM, applications in research Monday 7/21: LLMs and Research: Overview Liability and LLM (Ideally guest lawyer, if not, Tom) Thursday 7/24: Liability and LLM (Ideally guest lawyer, if not, Tom) Monday 7/28: Uncategorized Data: the Final Frontier (Tade, Olumide, Tom) Thursday 7/31: LLM and clinical practice: ups and downs.","title":"Table of contents"},{"location":"modules/module2/#key-learning-outcomes","text":"In this module we will get deeper into machine learning. We will focus on limitations and liability in LLM, applications in research, (uncategorized data, image analysis, etc), applications in clinical practice (communications/etc), and places where you should NOT use LLM.","title":"Key learning outcomes"},{"location":"modules/module2/#llms-and-research","text":"To be added.","title":"LLMS and Research"},{"location":"modules/module2/#questions","text":"To be added","title":"Questions?"},{"location":"modules/module2/#uncategorized-data","text":"To be added","title":"Uncategorized Data"},{"location":"modules/module2/#questions_1","text":"To be added Back to table of contents","title":"Questions?"},{"location":"modules/module2/#llms-in-the-clinic","text":"To be added","title":"LLMs in the Clinic"},{"location":"modules/module2/#questions_2","text":"To be added Back to table of contents","title":"Questions?"},{"location":"modules/module3/","text":"The Research Project The Research Project All students will submit a research project. This module will focus on generating an abstract worthy result.","title":"Module3"},{"location":"modules/module3/#the-research-project","text":"","title":"The Research Project"},{"location":"modules/module3/#the-research-project_1","text":"All students will submit a research project. This module will focus on generating an abstract worthy result.","title":"The Research Project"},{"location":"modules/module4/","text":"What is Machine Learning Table of contents Key learning outcomes ML/LLM Basic Relevant Biostatistcs Linux: Learn it, Love it ML/LLM Training - Image Analysis Key learning outcomes In this module we will focus on the basics - how to use linux, basic relevant biostatistical theory (data, statistical approaches, etc), and what machine learning is (and importantly, is not) conceptually. ML/LLM: The Basics To be added. Questions? To be added Basic Biostats To be added Questions? To be added Back to table of contents Linux Primer To be added Questions? To be added Back to table of contents ML/LLM Training To be added Questions? To be added Back to table of contents","title":"What is Machine Learning"},{"location":"modules/module4/#what-is-machine-learning","text":"","title":"What is Machine Learning"},{"location":"modules/module4/#table-of-contents","text":"Key learning outcomes ML/LLM Basic Relevant Biostatistcs Linux: Learn it, Love it ML/LLM Training - Image Analysis","title":"Table of contents"},{"location":"modules/module4/#key-learning-outcomes","text":"In this module we will focus on the basics - how to use linux, basic relevant biostatistical theory (data, statistical approaches, etc), and what machine learning is (and importantly, is not) conceptually.","title":"Key learning outcomes"},{"location":"modules/module4/#mlllm-the-basics","text":"To be added.","title":"ML/LLM: The Basics"},{"location":"modules/module4/#questions","text":"To be added","title":"Questions?"},{"location":"modules/module4/#basic-biostats","text":"To be added","title":"Basic Biostats"},{"location":"modules/module4/#questions_1","text":"To be added Back to table of contents","title":"Questions?"},{"location":"modules/module4/#linux-primer","text":"To be added","title":"Linux Primer"},{"location":"modules/module4/#questions_2","text":"To be added Back to table of contents","title":"Questions?"},{"location":"modules/module4/#mlllm-training","text":"To be added","title":"ML/LLM Training"},{"location":"modules/module4/#questions_3","text":"To be added Back to table of contents","title":"Questions?"},{"location":"modules/tade_save_backup_module/","text":"Population Genetics And Ancestry Analysis Table of Contents Key Learning Outcomes Practical Data A Portability Problem Population Genetics Basics Allele frequencies Linkage disequilibrium Dimensional Reduction: PCA Key Learning Outcomes After completing this practical, you should be able to: Run mixed ancestry PRS and understand the PRS Portability Problem. Understand principle component analysis and dimensional reduction. Understand basic population genetics and know how to analyze ancestry groups. Understand the challenges and limitations of applying PRS in populations with diverse genetic backgrounds. Practical Specific Data The data/software required for this practical can be found in the folder day1b . Inside of this folder there are two folders, one for each operating system. Please enter the correct directory, by typing one of the following two commands: cd day1b/mac_version # for users of macOs cd day1b/linux_version # for users of Linux After doing this you should see the following directories: exercise1: Includes Data/Code to run multi-ancestry PRS exercise2: Includes Data/Code for principal component analysis exercise3: Includes Data/Code for population genetics analysis The downloaded code in these directories needs to be made executable on your machine. You can do that now by typing: chmod +x exercise*/code/* Additionally, if you are using macOs where there is extra security, you may need to follow the additional instructions to allow your computer to run software here. Ex 1: Portability Problem The first exercise of this practical takes place in the folder exercise1 . Once inside the folder you should see code and data directories. Looking in the data directory by typing the following command will reveal: ls data/* \ud83c\uddea\ud83c\uddfa: EURO_GWAS.assoc (European Ancestry GWAS Sumstats) \ud83c\uddec\ud83c\udde7: data/ukTarget (Genotype Phenotype data From a population from the UK.) \ud83c\uddef\ud83c\uddf5: data/japanTarget (Genotype Phenotype data from a population from Japan.) To start, run PRSice using the European GWAS and target data from the UK: ./code/PRSice --base data/EURO_GWAS.assoc --target data/ukTarget/ukTarget --binary-target F --pheno data/ukTarget/ukTarget.pheno --out ukRun Verify that this command produce a file called \"ukRun.best\" that contains individual prs-scores in fourth column. This file can compared to the file data/ukTarget/ukTarget.pheno which contains phenotype-values in the third column. \ud83d\udea8 OPTIONAL-CHALLENGE \ud83d\udea8 Using R, Python, or another program, consider calculating the correlation between the PRS and phenotype data in the two files? First read the pseudocode and see if you can follow the strategy. Then give it a try or read the following solutions and make sure that you understand them. Notice the differences in similarities in the programming languages. Hints #1) Step1: Read Both Files in. prs_data = read('ukRun.best') pheno_data = read(\"data/ukTarget/ukTarget.pheno\") #2) Step2: Extract the correct Column from each file . prs_vals = extract_from(prs_data, column 4) pheno_vals = extract_from(pheno_data, column 3) #3) Step3: Calculate the correlation. R2 = calculate_R2_from_data(prs_vals, pheno_vals) Solution (R) R # read-in prs-file prs1 <- read.table(\"ukRun.best\", header = TRUE, sep = \"\", stringsAsFactors = FALSE) prs.data <- prs1[,4] # read-in pheno-file pheno <- read.table(\"data/ukTarget/ukTarget.pheno\", header = TRUE, sep = \"\", stringsAsFactors = FALSE) pheno.data <- pheno[,3] # Create DataFrame combined_data <- data.frame( x = prs.data, y = pheno.data) # Fit a linear model to the data model <- lm(y ~ x, data = combined_data) # Calculate the R-squared value r_squared <- summary(model)$r.squared # return R2 print(r_squared) Solution (Python) python3 # read-in prs-file: with open('ukRun.best') as F: prs_vals = [float(line.split()[-1]) for i,line in enumerate(F.readlines()) if i > 0] # read-in pheno-file: with open('data/ukTarget/ukTarget.pheno') as F: pheno_vals = [float(line.split()[-1]) for i,line in enumerate(F.readlines()) if i > 0] # calculate correlation prs_mean, pheno_mean = sum(prs_vals)/len(prs_vals), sum(pheno_vals)/len(pheno_vals) rTop = sum([(x-prs_mean)*(y-pheno_mean) for x,y in zip(prs_vals, pheno_vals)]) rBottom = (sum([(x-prs_mean)*(x-prs_mean) for x in prs_vals])**0.5) * (sum([(x-pheno_mean)*(x-pheno_mean) for x in pheno_vals])**0.5) # Return R2 R2 = (rTop/rBottom)*(rTop/rBottom) print(R2) After you feel confident about the code, please run the following Rscript in the code directory to calculate the correlation and create a scatterplot using the UK PRS-result: Rscript --vanilla code/plot_prs_results.R data/ukTarget/ukTarget.pheno ukRun.best This will create a scatterplot file called: ukRunScatterplot.pdf . Verify that you can view this pdf. Next type the commands below to reuse the European GWAS data and PRSice with the genotype-phenotype data from Japan. ./code/PRSice --base data/EURO_GWAS.assoc --target data/japanTarget/japanTarget --binary-target F --pheno data/japanTarget/japanTarget.pheno --out japanRun Rscript --vanilla code/plot_prs_results.R data/japanTarget/japanTarget.pheno japanRun.best View the resulting scatterplot and answer the questions below. \u2753QUESTIONS: In the UK-result, what percent of variance in phenotype is explained by prs? In the Japan-result, what percent of variance in phenotype is explained by prs? Besides a difference in variance explained, do you notice any other differences? What is the name of the problem that refers to this drop in performance? What are some causes of the problem? 1. Differences in LD. 2. Differences in allele frequency. 3. Differences in environment. 4. Differences in population-structure. Back to Top 1000 Genomes Data We have observed the PRS-portability problem in practice. Recall from the lecture that the primary drivers of the PRS portability problem are between population difference in allele frequency, linkage disequilibrium and effect sizes. In the next exercises we will use 1000 Genomes (1000G) data to compare allele frequencies and linkage disequilibrium across populations. The 1000G contains individuals from 26 source populations from five super-populations, Europe, East Asia, South Asian, Africa and America. Population structure and population assignment is often accomplished using principal components analysis (PCA). In the final exercise we learn what PCA is and how it can be used to separate population data by recent ancestry. Back to Top Ex 2: Population Genetics This exercise takes place in the folder exercise2 . Once inside the folder you should see code and data directories. Looking in the data directory by typing the following command: ls data/* \ud83c\udf0e: chr1-22.bed/bim/fam (Global Genotype Data) \ud83c\udff7\ufe0f: data/pop_info.pheno (Population specific annotation data) \ud83d\udcab: data/all_phase3.king.psam (Axillary Phase Data) Sample size of each super-population The first thing we would like to find out about this data is the number of individuals in each super-population. Type the following command to query the number of European ancestry individuals in the downloaded dataset: grep -F \"EUR\" data/all_phase3.king.psam | wc -l Next, repeat the same command for East Asian, African, South Asian and American super-populations, by inserting the relevant ancestry codes (EAS, AFR, SAS, AMR). \ud83d\uddd2\ufe0f Make a note of how many individuals there are in each super-population. Number of genetic variants analysed 1000G data contains over 80 million variants genome-wide. The 1000G data we are using in this practical is only a small fraction of these variants. This data gives a reliable approximation for the genomic analyses in this tutorial and importantly, reduces the computation time required to complete the tutorial. The following command counts the number of genetic variants on chromosomes 1-22 used in our analyses wc -l data/chr1-22.bim Number of polymorphic markers in each super-populations The rate at which a genetic variant occurs in a population is known as its allele frequency. Allele frequencies are shaped by evolutionary forces over a long period of time and hence can vary between populations. This has implications for PRS research. The following plink command uses the population information in the file pop_info.pheno to generate allele frequency statistics for each SNP in the five 1000G super-populations: ./code/plink --bfile data/chr1-22 --snps-only --freq --within data/pop_info.pheno Population-stratified allele frequency results are found in the output file plink.frq.strat. Compare the totals against number of SNPs which have minor allele frequencies greater than 0 (and hence are useful for statistical analysis). Do this for all 5 populations (EAS, EUR, SAS, EUR and AFR), using the code below: grep -F \"AFR\" plink.frq.strat > freq_report.afr grep -F \"AMR\" plink.frq.strat > freq_report.amr grep -F \"EUR\" plink.frq.strat > freq_report.eur grep -F \"EAS\" plink.frq.strat > freq_report.eas grep -F \"SAS\" plink.frq.strat > freq_report.sas grep -F \"AFR\" plink.frq.strat | awk '$6 >0' freq_report.afr | wc -l grep -F \"EUR\" plink.frq.strat | awk '$6 >0' freq_report.eur | wc -l grep -F \"EAS\" plink.frq.strat | awk '$6 >0' freq_report.eas | wc -l grep -F \"AMR\" plink.frq.strat | awk '$6 >0' freq_report.amr | wc -l grep -F \"SAS\" plink.frq.strat | awk '$6 >0' freq_report.sas | wc -l Having compared the number of SNPs that show variation in each population, answer the following questions: \u2753QUESTIONS: Which populations have the largest number (density) of SNPs that can be considered polymorphic? What do you think is the significance of the observed population order? Allele frequency variation across the super-populations Here we compare the distribution of allele frequency in the five ancestral populations. To do this we will use the previously-generated output on minor allele frequencies per ancestry group (the file \"plink.frq.strat\"), using R: R-Code: Compare Allele Frequencies library(dplyr) library(ggplot2) freq <-read.table(\"plink.frq.strat\", header =T) plotDat <- freq %>% mutate(AlleleFrequency = cut(MAF, seq(0, 1, 0.25))) %>% group_by(AlleleFrequency, CLST) %>% summarise(FractionOfSNPs = n()/nrow(freq) * 100) ggplot(na.omit(plotDat),aes(AlleleFrequency, FractionOfSNPs, group = CLST, col = CLST)) + geom_line() + scale_y_continuous(limits = c(0, 12)) + ggtitle(\"Distribution of allele frequency across genome\") \u2753QUESTIONS: How are the allele frequencies in AFR distinguishable from the other global reference groups? Linkage disequilibrium variation across populations We will now perform pairwise LD comparisons between genome-wide SNPs to show how the relationship between genomic distance and LD strength varies between populations. We first derive information on pairwise R2 between all SNPs: ./code/plink --bfile data/chr1-22 --keep-cluster-names AFR --within data/pop_info.pheno --r2 --ld-window-r2 0 --ld-window 999999 --ld-window-kb 2500 --threads 30 --out chr1-22.AFR Repeat this step for all five 1000G populations. Output files containing LD info for all pairwise SNPs, have a \u2018.ld\u2019 su\ufb03x Next, create a summary file containing the base-pair distance between each pair and the corresponding R2 value. The following example shows this for AFR and EUR populations only, as just these populations will be used in the plot. cat chr1-22.AFR.ld | sed 1,1d | awk -F \" \" 'function abs(v) {return v < 0 ? -v : v}BEGIN{OFS=\"\\t\"}{print abs($5-$2),$7}' | sort -k1,1n > chr1-22.AFR.ld.summary cat chr1-22.EUR.ld | sed 1,1d | awk -F \" \" 'function abs(v) {return v < 0 ? -v : v}BEGIN{OFS=\"\\t\"}{print abs($5-$2),$7}' | sort -k1,1n > chr1-22.EUR.ld.summary LD decay versus chromosomal distance The following R commamds use this output to display LD as a function of genomic distance for the African and European populations: R-Code: Visualize LD Behavior # need to add additional functionality to be able to # carry out the necessary data transformation (dplyr) # and manipulation of character strings (stringr ) install.packages(\"dplyr\") install.packages(\"stringr\") install.packages(\"ggplot2\") library(dplyr) library(stringr) library(ggplot2) # Next we will (1) load the previously generated information on pairwise LD, # Categorize distances into intervals of fixed length (100KB), # Compute mean and median r2 within blocks # Obrain mid-points for each distance interval dfr<-read.delim(\"chr1-22.AFR.ld.summary\",sep=\"\",header=F,check.names=F, stringsAsFactors=F) colnames(dfr)<-c(\"dist\",\"rsq\") dfr$distc<-cut(dfr$dist,breaks=seq(from=min(dfr$dist)-1,to=max(dfr$dist)+1,by=100000)) dfr1<-dfr %>% group_by(distc) %>% summarise(mean=mean(rsq),median=median(rsq)) dfr1 <- dfr1 %>% mutate(start=as.integer(str_extract(str_replace_all(distc,\"[\\\\(\\\\)\\\\[\\\\]]\",\"\"),\"^[0-9-e+.]+\")), end=as.integer(str_extract(str_replace_all(distc,\"[\\\\(\\\\)\\\\[\\\\]]\",\"\"),\"[0-9-e+.]+$\")), mid=start+((end-start)/2)) # The preceding code block should be repeated for the file chr1-22._EUR.ld.summary. # When doing so, the output object dfr1 on lines 4 and 5 should be renamed dfr2 to prevent the object df1 being over-written. # Finally, we can plot LD decay for AFR and EUR reference populations in a single graph: ggplot()+ geom_point(data=dfr1,aes(x=start,y=mean),size=0.4,colour=\"grey20\")+ geom_line(data=dfr1,aes(x=start,y=mean),size=0.3,alpha=0.5,colour=\"grey40\")+ labs(x=\"Distance (Megabases)\",y=expression(LD~(r^{2})))+ scale_x_continuous(breaks=c(0,2*10^6,4*10^6,6*10^6,8*10^6),labels=c(\"0\",\"2\",\"4\",\"6\",\"8\"))+ theme_bw() \u2753QUESTIONS: What differences do you observe in terms of LD decay between AFR and EUR genomes? How is this likely to impact the transferability of PRS performance between the two populations? Distribution of LD-block length The next set of scripts will allow us to visualise the distribution of LD block length in the 1000G super-populations. ./code/plink --bfile data/chr1-22 --keep-cluster-names AFR --blocks no-pheno-req no-small-max-span --blocks-max-kb 250 --within data/pop_info.pheno --threads 30 --out AFR The \u201c\u2013block\" flag estimates haplotype blocks using the same block definition implemented by the software Haploview. The default setting for the flag --blocks-max-kb only considers pairs of variants that are within 200 kilobases of each other. The output file from the above command is a .blocks file. Use the same code to generate output for EAS, EUR, SAS and AMR populations (as it is not possible to generate population-specific information using the --within flag). Then, in R: R-Code: Load each of the 5 datasets and set column names to lower case. dfr.afr <- read.delim(\"AFR.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.afr) <- tolower(colnames(dfr.afr)) dfr.eur <- read.delim(\"EUR.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.eur) <- tolower(colnames(dfr.eur)) dfr.amr <- read.delim(\"AMR.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.amr) <- tolower(colnames(dfr.amr)) dfr.sas <- read.delim(\"SAS.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.sas) <- tolower(colnames(dfr.sas)) dfr.eas <- read.delim(\"EAS.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.eas) <- tolower(colnames(dfr.eas)) Then plot the data: plot (density(dfr.afr$kb), main=\"LD block length distribution\", ylab=\"Density\",xlab=\"LD block length (Kb)\" ) lines (density(dfr.eur$kb), col=\"blue\") lines (density(dfr.eas$kb), col=\"red\") lines (density(dfr.amr$kb), col=\"purple\") lines (density(dfr.sas$kb), col=\"green\") legend(\"topright\",c(\"AFR\",\"EAS\",\"EUR\",\"SAS\",\"AMR\"), fill=c(\"black\",\"red\",\"blue\",\"green\",\"purple\")) \u2753QUESTIONS: What are the main features of this plot? How do you interpret them? Back to Top Ex 3: PCA Principle component analysis is a useful technique that allows researchers to visualize high dimensional data in lower space by rotating the axes in such a way that the lower dimensions (or components) maximize the total variance explained. In statistical genetics this involves \"rotating\" million-dimensional data - something that is very hard to visualize! For this reason, we begin with a simpler exercise. For the following three two dimensional shapes, spend some time identifying the principle components or sketching the line across for which variance is maximized. Check your answers below: \u2753QUESTIONS: What line represents the principle component for the first shape? What line represents the principle component for the second shape? What line represents the principle component for the third shape? Below you can view the shapes in principal component space. Now that we understand how PCA works in two dimensions we will consider a higher dimensional example. In the three dimensional space below, see if you can visualize a plane that maximizes the variance across two dimensions: Did you get it right? If so, realize that this is equivalent to what we do in genetics - we find rotate the data through millions of dimensions of space to find the plane that maximizes the variance in two dimensions: To run PCA with real data please enter the exercise3 directory, and type the following command to run PCA on the 1000 Genome data: ./code/plink --bfile data/chr1-22 --indep-pairwise 250 25 0.1 --maf 0.1 --threads 30 --out chr1-22.ldpruned_all_1kgv2 ./code/plink --bfile data/chr1-22 --extract chr1-22.ldpruned_all_1kgv2.prune.in --pca --threads 30 This will generate the principal components that maximize the variance in the data. To plot the result run the following commands from with an R-terminal: R-Code: Generate a PCA Plot require('RColorBrewer') options(scipen=100, digits=3) eigenvec <- read.table('plink.eigenvec', header = F, skip=0, sep = ' ') rownames(eigenvec) <- eigenvec[,2] eigenvec <- eigenvec[,3:ncol(eigenvec)] colnames(eigenvec) <- paste('Principal Component ', c(1:20), sep = '') PED <- read.table(\"data/all_phase3.king.psam\", header = TRUE, skip = 0, sep = '\\t') PED <- PED[which(PED$IID %in% rownames(eigenvec)), ] PED <- PED[match(rownames(eigenvec), PED$IID),] PED$Population <- factor(PED$Population, levels=c(\"ACB\",\"ASW\",\"ESN\",\"GWD\",\"LWK\",\"MSL\",\"YRI\",\"CLM\",\"MXL\",\"PEL\",\"PUR\",\"CDX\",\"CHB\",\"CHS\",\"JPT\",\"KHV\",\"CEU\",\"FIN\",\"GBR\",\"IBS\",\"TSI\",\"BEB\",\"GIH\",\"ITU\",\"PJL\",\"STU\")) col <- colorRampPalette(c(\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"forestgreen\",\"forestgreen\",\"forestgreen\",\"forestgreen\",\"grey\",\"grey\",\"grey\",\"grey\",\"grey\", \"royalblue\",\"royalblue\",\"royalblue\",\"royalblue\",\"royalblue\",\"black\",\"black\",\"black\",\"black\",\"black\"))(length(unique(PED$Population)))[factor(PED$Population)] project.pca <- eigenvec par(mar = c(5,5,5,5), cex = 2.0,cex.main = 7, cex.axis = 2.75, cex.lab = 2.75, mfrow = c(1,2)) plot(project.pca[,1], project.pca[,2], type = 'n', main = 'A', adj = 0.5, xlab = 'First component', ylab = 'Second component', font = 2, font.lab = 2) points(project.pca[,1], project.pca[,2], col = col, pch = 20, cex = 2.25) legend('bottomright', bty = 'n', cex = 3.0, title = '', c('AFR', 'AMR', 'EAS', 'EUR', 'SAS'), fill = c('yellow', 'forestgreen', 'grey', 'royalblue', 'black')) plot(project.pca[,1], project.pca[,3], type=\"n\", main=\"B\", adj=0.5, xlab=\"First component\", ylab=\"Third component\", font=2, font.lab=2) points(project.pca[,1], project.pca[,3], col=col, pch=20, cex=2.25) \u2753QUESTIONS: What is distinct about the PC projections of the AMR group relative to other populations? Why does this occur? What does it tell us about ancestry of this group?","title":"Population Genetics And Ancestry Analysis"},{"location":"modules/tade_save_backup_module/#population-genetics-and-ancestry-analysis","text":"","title":"Population Genetics And Ancestry Analysis"},{"location":"modules/tade_save_backup_module/#table-of-contents","text":"Key Learning Outcomes Practical Data A Portability Problem Population Genetics Basics Allele frequencies Linkage disequilibrium Dimensional Reduction: PCA","title":"Table of Contents"},{"location":"modules/tade_save_backup_module/#key-learning-outcomes","text":"After completing this practical, you should be able to: Run mixed ancestry PRS and understand the PRS Portability Problem. Understand principle component analysis and dimensional reduction. Understand basic population genetics and know how to analyze ancestry groups. Understand the challenges and limitations of applying PRS in populations with diverse genetic backgrounds.","title":"Key Learning Outcomes"},{"location":"modules/tade_save_backup_module/#ex-1-portability-problem","text":"The first exercise of this practical takes place in the folder exercise1 . Once inside the folder you should see code and data directories. Looking in the data directory by typing the following command will reveal: ls data/* \ud83c\uddea\ud83c\uddfa: EURO_GWAS.assoc (European Ancestry GWAS Sumstats) \ud83c\uddec\ud83c\udde7: data/ukTarget (Genotype Phenotype data From a population from the UK.) \ud83c\uddef\ud83c\uddf5: data/japanTarget (Genotype Phenotype data from a population from Japan.) To start, run PRSice using the European GWAS and target data from the UK: ./code/PRSice --base data/EURO_GWAS.assoc --target data/ukTarget/ukTarget --binary-target F --pheno data/ukTarget/ukTarget.pheno --out ukRun Verify that this command produce a file called \"ukRun.best\" that contains individual prs-scores in fourth column. This file can compared to the file data/ukTarget/ukTarget.pheno which contains phenotype-values in the third column. \ud83d\udea8 OPTIONAL-CHALLENGE \ud83d\udea8 Using R, Python, or another program, consider calculating the correlation between the PRS and phenotype data in the two files? First read the pseudocode and see if you can follow the strategy. Then give it a try or read the following solutions and make sure that you understand them. Notice the differences in similarities in the programming languages. Hints #1) Step1: Read Both Files in. prs_data = read('ukRun.best') pheno_data = read(\"data/ukTarget/ukTarget.pheno\") #2) Step2: Extract the correct Column from each file . prs_vals = extract_from(prs_data, column 4) pheno_vals = extract_from(pheno_data, column 3) #3) Step3: Calculate the correlation. R2 = calculate_R2_from_data(prs_vals, pheno_vals) Solution (R) R # read-in prs-file prs1 <- read.table(\"ukRun.best\", header = TRUE, sep = \"\", stringsAsFactors = FALSE) prs.data <- prs1[,4] # read-in pheno-file pheno <- read.table(\"data/ukTarget/ukTarget.pheno\", header = TRUE, sep = \"\", stringsAsFactors = FALSE) pheno.data <- pheno[,3] # Create DataFrame combined_data <- data.frame( x = prs.data, y = pheno.data) # Fit a linear model to the data model <- lm(y ~ x, data = combined_data) # Calculate the R-squared value r_squared <- summary(model)$r.squared # return R2 print(r_squared) Solution (Python) python3 # read-in prs-file: with open('ukRun.best') as F: prs_vals = [float(line.split()[-1]) for i,line in enumerate(F.readlines()) if i > 0] # read-in pheno-file: with open('data/ukTarget/ukTarget.pheno') as F: pheno_vals = [float(line.split()[-1]) for i,line in enumerate(F.readlines()) if i > 0] # calculate correlation prs_mean, pheno_mean = sum(prs_vals)/len(prs_vals), sum(pheno_vals)/len(pheno_vals) rTop = sum([(x-prs_mean)*(y-pheno_mean) for x,y in zip(prs_vals, pheno_vals)]) rBottom = (sum([(x-prs_mean)*(x-prs_mean) for x in prs_vals])**0.5) * (sum([(x-pheno_mean)*(x-pheno_mean) for x in pheno_vals])**0.5) # Return R2 R2 = (rTop/rBottom)*(rTop/rBottom) print(R2) After you feel confident about the code, please run the following Rscript in the code directory to calculate the correlation and create a scatterplot using the UK PRS-result: Rscript --vanilla code/plot_prs_results.R data/ukTarget/ukTarget.pheno ukRun.best This will create a scatterplot file called: ukRunScatterplot.pdf . Verify that you can view this pdf. Next type the commands below to reuse the European GWAS data and PRSice with the genotype-phenotype data from Japan. ./code/PRSice --base data/EURO_GWAS.assoc --target data/japanTarget/japanTarget --binary-target F --pheno data/japanTarget/japanTarget.pheno --out japanRun Rscript --vanilla code/plot_prs_results.R data/japanTarget/japanTarget.pheno japanRun.best View the resulting scatterplot and answer the questions below. \u2753QUESTIONS: In the UK-result, what percent of variance in phenotype is explained by prs? In the Japan-result, what percent of variance in phenotype is explained by prs? Besides a difference in variance explained, do you notice any other differences? What is the name of the problem that refers to this drop in performance? What are some causes of the problem? 1. Differences in LD. 2. Differences in allele frequency. 3. Differences in environment. 4. Differences in population-structure. Back to Top","title":"Ex 1: Portability Problem"},{"location":"modules/tade_save_backup_module/#1000-genomes-data","text":"We have observed the PRS-portability problem in practice. Recall from the lecture that the primary drivers of the PRS portability problem are between population difference in allele frequency, linkage disequilibrium and effect sizes. In the next exercises we will use 1000 Genomes (1000G) data to compare allele frequencies and linkage disequilibrium across populations. The 1000G contains individuals from 26 source populations from five super-populations, Europe, East Asia, South Asian, Africa and America. Population structure and population assignment is often accomplished using principal components analysis (PCA). In the final exercise we learn what PCA is and how it can be used to separate population data by recent ancestry. Back to Top","title":"1000 Genomes Data"},{"location":"modules/tade_save_backup_module/#ex-2-population-genetics","text":"This exercise takes place in the folder exercise2 . Once inside the folder you should see code and data directories. Looking in the data directory by typing the following command: ls data/* \ud83c\udf0e: chr1-22.bed/bim/fam (Global Genotype Data) \ud83c\udff7\ufe0f: data/pop_info.pheno (Population specific annotation data) \ud83d\udcab: data/all_phase3.king.psam (Axillary Phase Data)","title":"Ex 2: Population Genetics"},{"location":"modules/tade_save_backup_module/#sample-size-of-each-super-population","text":"The first thing we would like to find out about this data is the number of individuals in each super-population. Type the following command to query the number of European ancestry individuals in the downloaded dataset: grep -F \"EUR\" data/all_phase3.king.psam | wc -l Next, repeat the same command for East Asian, African, South Asian and American super-populations, by inserting the relevant ancestry codes (EAS, AFR, SAS, AMR). \ud83d\uddd2\ufe0f Make a note of how many individuals there are in each super-population.","title":"Sample size of each super-population"},{"location":"modules/tade_save_backup_module/#number-of-genetic-variants-analysed","text":"1000G data contains over 80 million variants genome-wide. The 1000G data we are using in this practical is only a small fraction of these variants. This data gives a reliable approximation for the genomic analyses in this tutorial and importantly, reduces the computation time required to complete the tutorial. The following command counts the number of genetic variants on chromosomes 1-22 used in our analyses wc -l data/chr1-22.bim","title":"Number of genetic variants analysed"},{"location":"modules/tade_save_backup_module/#number-of-polymorphic-markers-in-each-super-populations","text":"The rate at which a genetic variant occurs in a population is known as its allele frequency. Allele frequencies are shaped by evolutionary forces over a long period of time and hence can vary between populations. This has implications for PRS research. The following plink command uses the population information in the file pop_info.pheno to generate allele frequency statistics for each SNP in the five 1000G super-populations: ./code/plink --bfile data/chr1-22 --snps-only --freq --within data/pop_info.pheno Population-stratified allele frequency results are found in the output file plink.frq.strat. Compare the totals against number of SNPs which have minor allele frequencies greater than 0 (and hence are useful for statistical analysis). Do this for all 5 populations (EAS, EUR, SAS, EUR and AFR), using the code below: grep -F \"AFR\" plink.frq.strat > freq_report.afr grep -F \"AMR\" plink.frq.strat > freq_report.amr grep -F \"EUR\" plink.frq.strat > freq_report.eur grep -F \"EAS\" plink.frq.strat > freq_report.eas grep -F \"SAS\" plink.frq.strat > freq_report.sas grep -F \"AFR\" plink.frq.strat | awk '$6 >0' freq_report.afr | wc -l grep -F \"EUR\" plink.frq.strat | awk '$6 >0' freq_report.eur | wc -l grep -F \"EAS\" plink.frq.strat | awk '$6 >0' freq_report.eas | wc -l grep -F \"AMR\" plink.frq.strat | awk '$6 >0' freq_report.amr | wc -l grep -F \"SAS\" plink.frq.strat | awk '$6 >0' freq_report.sas | wc -l Having compared the number of SNPs that show variation in each population, answer the following questions: \u2753QUESTIONS: Which populations have the largest number (density) of SNPs that can be considered polymorphic? What do you think is the significance of the observed population order?","title":"Number of polymorphic markers in each super-populations"},{"location":"modules/tade_save_backup_module/#allele-frequency-variation-across-the-super-populations","text":"Here we compare the distribution of allele frequency in the five ancestral populations. To do this we will use the previously-generated output on minor allele frequencies per ancestry group (the file \"plink.frq.strat\"), using R: R-Code: Compare Allele Frequencies library(dplyr) library(ggplot2) freq <-read.table(\"plink.frq.strat\", header =T) plotDat <- freq %>% mutate(AlleleFrequency = cut(MAF, seq(0, 1, 0.25))) %>% group_by(AlleleFrequency, CLST) %>% summarise(FractionOfSNPs = n()/nrow(freq) * 100) ggplot(na.omit(plotDat),aes(AlleleFrequency, FractionOfSNPs, group = CLST, col = CLST)) + geom_line() + scale_y_continuous(limits = c(0, 12)) + ggtitle(\"Distribution of allele frequency across genome\") \u2753QUESTIONS: How are the allele frequencies in AFR distinguishable from the other global reference groups?","title":"Allele frequency variation across the super-populations"},{"location":"modules/tade_save_backup_module/#linkage-disequilibrium-variation-across-populations","text":"We will now perform pairwise LD comparisons between genome-wide SNPs to show how the relationship between genomic distance and LD strength varies between populations. We first derive information on pairwise R2 between all SNPs: ./code/plink --bfile data/chr1-22 --keep-cluster-names AFR --within data/pop_info.pheno --r2 --ld-window-r2 0 --ld-window 999999 --ld-window-kb 2500 --threads 30 --out chr1-22.AFR Repeat this step for all five 1000G populations. Output files containing LD info for all pairwise SNPs, have a \u2018.ld\u2019 su\ufb03x Next, create a summary file containing the base-pair distance between each pair and the corresponding R2 value. The following example shows this for AFR and EUR populations only, as just these populations will be used in the plot. cat chr1-22.AFR.ld | sed 1,1d | awk -F \" \" 'function abs(v) {return v < 0 ? -v : v}BEGIN{OFS=\"\\t\"}{print abs($5-$2),$7}' | sort -k1,1n > chr1-22.AFR.ld.summary cat chr1-22.EUR.ld | sed 1,1d | awk -F \" \" 'function abs(v) {return v < 0 ? -v : v}BEGIN{OFS=\"\\t\"}{print abs($5-$2),$7}' | sort -k1,1n > chr1-22.EUR.ld.summary","title":"Linkage disequilibrium variation across populations"},{"location":"modules/tade_save_backup_module/#ld-decay-versus-chromosomal-distance","text":"The following R commamds use this output to display LD as a function of genomic distance for the African and European populations: R-Code: Visualize LD Behavior # need to add additional functionality to be able to # carry out the necessary data transformation (dplyr) # and manipulation of character strings (stringr ) install.packages(\"dplyr\") install.packages(\"stringr\") install.packages(\"ggplot2\") library(dplyr) library(stringr) library(ggplot2) # Next we will (1) load the previously generated information on pairwise LD, # Categorize distances into intervals of fixed length (100KB), # Compute mean and median r2 within blocks # Obrain mid-points for each distance interval dfr<-read.delim(\"chr1-22.AFR.ld.summary\",sep=\"\",header=F,check.names=F, stringsAsFactors=F) colnames(dfr)<-c(\"dist\",\"rsq\") dfr$distc<-cut(dfr$dist,breaks=seq(from=min(dfr$dist)-1,to=max(dfr$dist)+1,by=100000)) dfr1<-dfr %>% group_by(distc) %>% summarise(mean=mean(rsq),median=median(rsq)) dfr1 <- dfr1 %>% mutate(start=as.integer(str_extract(str_replace_all(distc,\"[\\\\(\\\\)\\\\[\\\\]]\",\"\"),\"^[0-9-e+.]+\")), end=as.integer(str_extract(str_replace_all(distc,\"[\\\\(\\\\)\\\\[\\\\]]\",\"\"),\"[0-9-e+.]+$\")), mid=start+((end-start)/2)) # The preceding code block should be repeated for the file chr1-22._EUR.ld.summary. # When doing so, the output object dfr1 on lines 4 and 5 should be renamed dfr2 to prevent the object df1 being over-written. # Finally, we can plot LD decay for AFR and EUR reference populations in a single graph: ggplot()+ geom_point(data=dfr1,aes(x=start,y=mean),size=0.4,colour=\"grey20\")+ geom_line(data=dfr1,aes(x=start,y=mean),size=0.3,alpha=0.5,colour=\"grey40\")+ labs(x=\"Distance (Megabases)\",y=expression(LD~(r^{2})))+ scale_x_continuous(breaks=c(0,2*10^6,4*10^6,6*10^6,8*10^6),labels=c(\"0\",\"2\",\"4\",\"6\",\"8\"))+ theme_bw() \u2753QUESTIONS: What differences do you observe in terms of LD decay between AFR and EUR genomes? How is this likely to impact the transferability of PRS performance between the two populations?","title":"LD decay versus chromosomal distance"},{"location":"modules/tade_save_backup_module/#distribution-of-ld-block-length","text":"The next set of scripts will allow us to visualise the distribution of LD block length in the 1000G super-populations. ./code/plink --bfile data/chr1-22 --keep-cluster-names AFR --blocks no-pheno-req no-small-max-span --blocks-max-kb 250 --within data/pop_info.pheno --threads 30 --out AFR The \u201c\u2013block\" flag estimates haplotype blocks using the same block definition implemented by the software Haploview. The default setting for the flag --blocks-max-kb only considers pairs of variants that are within 200 kilobases of each other. The output file from the above command is a .blocks file. Use the same code to generate output for EAS, EUR, SAS and AMR populations (as it is not possible to generate population-specific information using the --within flag). Then, in R: R-Code: Load each of the 5 datasets and set column names to lower case. dfr.afr <- read.delim(\"AFR.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.afr) <- tolower(colnames(dfr.afr)) dfr.eur <- read.delim(\"EUR.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.eur) <- tolower(colnames(dfr.eur)) dfr.amr <- read.delim(\"AMR.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.amr) <- tolower(colnames(dfr.amr)) dfr.sas <- read.delim(\"SAS.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.sas) <- tolower(colnames(dfr.sas)) dfr.eas <- read.delim(\"EAS.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.eas) <- tolower(colnames(dfr.eas)) Then plot the data: plot (density(dfr.afr$kb), main=\"LD block length distribution\", ylab=\"Density\",xlab=\"LD block length (Kb)\" ) lines (density(dfr.eur$kb), col=\"blue\") lines (density(dfr.eas$kb), col=\"red\") lines (density(dfr.amr$kb), col=\"purple\") lines (density(dfr.sas$kb), col=\"green\") legend(\"topright\",c(\"AFR\",\"EAS\",\"EUR\",\"SAS\",\"AMR\"), fill=c(\"black\",\"red\",\"blue\",\"green\",\"purple\")) \u2753QUESTIONS: What are the main features of this plot? How do you interpret them? Back to Top","title":"Distribution of LD-block length"},{"location":"modules/tade_save_backup_module/#ex-3-pca","text":"Principle component analysis is a useful technique that allows researchers to visualize high dimensional data in lower space by rotating the axes in such a way that the lower dimensions (or components) maximize the total variance explained. In statistical genetics this involves \"rotating\" million-dimensional data - something that is very hard to visualize! For this reason, we begin with a simpler exercise. For the following three two dimensional shapes, spend some time identifying the principle components or sketching the line across for which variance is maximized. Check your answers below: \u2753QUESTIONS: What line represents the principle component for the first shape? What line represents the principle component for the second shape? What line represents the principle component for the third shape? Below you can view the shapes in principal component space. Now that we understand how PCA works in two dimensions we will consider a higher dimensional example. In the three dimensional space below, see if you can visualize a plane that maximizes the variance across two dimensions: Did you get it right? If so, realize that this is equivalent to what we do in genetics - we find rotate the data through millions of dimensions of space to find the plane that maximizes the variance in two dimensions: To run PCA with real data please enter the exercise3 directory, and type the following command to run PCA on the 1000 Genome data: ./code/plink --bfile data/chr1-22 --indep-pairwise 250 25 0.1 --maf 0.1 --threads 30 --out chr1-22.ldpruned_all_1kgv2 ./code/plink --bfile data/chr1-22 --extract chr1-22.ldpruned_all_1kgv2.prune.in --pca --threads 30 This will generate the principal components that maximize the variance in the data. To plot the result run the following commands from with an R-terminal: R-Code: Generate a PCA Plot require('RColorBrewer') options(scipen=100, digits=3) eigenvec <- read.table('plink.eigenvec', header = F, skip=0, sep = ' ') rownames(eigenvec) <- eigenvec[,2] eigenvec <- eigenvec[,3:ncol(eigenvec)] colnames(eigenvec) <- paste('Principal Component ', c(1:20), sep = '') PED <- read.table(\"data/all_phase3.king.psam\", header = TRUE, skip = 0, sep = '\\t') PED <- PED[which(PED$IID %in% rownames(eigenvec)), ] PED <- PED[match(rownames(eigenvec), PED$IID),] PED$Population <- factor(PED$Population, levels=c(\"ACB\",\"ASW\",\"ESN\",\"GWD\",\"LWK\",\"MSL\",\"YRI\",\"CLM\",\"MXL\",\"PEL\",\"PUR\",\"CDX\",\"CHB\",\"CHS\",\"JPT\",\"KHV\",\"CEU\",\"FIN\",\"GBR\",\"IBS\",\"TSI\",\"BEB\",\"GIH\",\"ITU\",\"PJL\",\"STU\")) col <- colorRampPalette(c(\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"forestgreen\",\"forestgreen\",\"forestgreen\",\"forestgreen\",\"grey\",\"grey\",\"grey\",\"grey\",\"grey\", \"royalblue\",\"royalblue\",\"royalblue\",\"royalblue\",\"royalblue\",\"black\",\"black\",\"black\",\"black\",\"black\"))(length(unique(PED$Population)))[factor(PED$Population)] project.pca <- eigenvec par(mar = c(5,5,5,5), cex = 2.0,cex.main = 7, cex.axis = 2.75, cex.lab = 2.75, mfrow = c(1,2)) plot(project.pca[,1], project.pca[,2], type = 'n', main = 'A', adj = 0.5, xlab = 'First component', ylab = 'Second component', font = 2, font.lab = 2) points(project.pca[,1], project.pca[,2], col = col, pch = 20, cex = 2.25) legend('bottomright', bty = 'n', cex = 3.0, title = '', c('AFR', 'AMR', 'EAS', 'EUR', 'SAS'), fill = c('yellow', 'forestgreen', 'grey', 'royalblue', 'black')) plot(project.pca[,1], project.pca[,3], type=\"n\", main=\"B\", adj=0.5, xlab=\"First component\", ylab=\"Third component\", font=2, font.lab=2) points(project.pca[,1], project.pca[,3], col=col, pch=20, cex=2.25) \u2753QUESTIONS: What is distinct about the PC projections of the AMR group relative to other populations? Why does this occur? What does it tell us about ancestry of this group?","title":"Ex 3: PCA"}]}